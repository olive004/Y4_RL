{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "# @title Grid world code\n",
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [-10, -10, -10, -10, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "\n",
    "        # Measuring convergence via delta's\n",
    "        self.delta_history = []\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        # Measuring convergence via delta's\n",
    "        self.delta_history = []\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            self.delta_history.append(delta)\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "\n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        # plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "    def policty_iteration(self, values, Policy):\n",
    "        \"\"\" Improvement of policy. Not finished \"\"\"\n",
    "        is_policy_stable = True\n",
    "        for state_idx in range(policy.shape[0]):\n",
    "            temp_policy = policy\n",
    "\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "        \n",
    "def plot_value_function(V, ax=None, show=True):\n",
    "    \"\"\"Plot V(s), the value function\"\"\"\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    ax.stem(V, use_line_collection=True)\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_xlabel('State')\n",
    "    ax.set_title(\"Value function: $V(s)$\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_stem(variable, ax=None, show=True):\n",
    "    \"\"\"Plot any variable like the value function\"\"\"\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    ax.stem(variable, use_line_collection=True)\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_xlabel('State')\n",
    "    ax.set_title(\"Value function: $V(s)$\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "        "
   ],
   "cell_type": "code",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "source": [
    "# Question 1\n",
    "\n",
    "1. For a uniform policy (with equal probability of choosing any action), use the implementation of\n",
    "Policy Evaluation (policy evaluation) to evaluate the state-values for that policy with γ = 0.9.\n",
    "Investigate how the state-values change for different values of γ. Think of a way to measure\n",
    "convergence, and investigate how the rate of convergence of policy evaluation depends on γ.\n",
    "Can you explain why?"
   ],
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "source": [
    "### 1. Evaluate the state-values for that policy with γ = 0.9"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating the Grid world, represented as:\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 362,
       "height": 129
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "### Define the grid\n",
    "print(\"Creating the Grid world, represented as:\\n\")\n",
    "grid = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2ccadf8fe32d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Policy is : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_gamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Change here!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The value of the policy for 0.9 is : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "discount_gamma = 0.9\n",
    "threshold = 0.001\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.ones((grid.state_size, grid.action_size)) / grid.action_size\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy, threshold, discount_gamma) #Change here!\n",
    "print(\"The value of the policy for 0.9 is : {}\".format(val))\n",
    "\n",
    "plot_value_function(val)\n",
    "grid.draw_value(val)\n",
    "\n",
    "print(\"\\nIt took {} epochs\\n\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Value function widget\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the interactive plot!\n",
    "\n",
    "@widgets.interact\n",
    "def plot_value_fn_theta_gamma(threshold_theta = widgets.FloatSlider(value=0.0001, min=0.001, max=1.1, step=0.001, readout_format='.4f', description=\"threshold\"),\n",
    "                              discount_gamma = widgets.FloatSlider(value=0.9, min=0.001, max=1.0, step=0.001, readout_format='.3f', description=\"discount\")):\n",
    "  \n",
    "  try:\n",
    "    V = grid.policy_evaluation(Policy, threshold_theta, discount_gamma)\n",
    "  except NotImplementedError:\n",
    "    print(\"Complete Question 1 to enable this interactive demo\")\n",
    "  \n",
    "  plot_value_function(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Measure convergence\n",
    "\n",
    "\n",
    "@widgets.interact\n",
    "def plot_convergence(threshold_theta = widgets.FloatSlider(value=0.0001, min=0.001, max=1.1, step=0.001, readout_format='.4f', description=\"threshold\"),\n",
    "                        discount_gamma = widgets.FloatSlider(value=0.9, min=0.001, max=1.0, step=0.001, readout_format='.3f', description=\"discount\")):\n",
    "  \n",
    "    try:\n",
    "        V = grid.policy_evaluation(Policy, threshold_theta, discount_gamma)\n",
    "        deltas = grid.delta_history\n",
    "    except NotImplementedError:\n",
    "        print(\"Complete Question 1 to enable this interactive demo\")\n",
    "  \n",
    "    plot_stem(deltas)\n"
   ]
  },
  {
   "source": [
    "### How the rate of convergence of policy evaluation depends on γ\n",
    "As the discount decreases, "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 362,
       "height": 129
      },
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Policy is : [[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\nThe value of that policy is : 0\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val = 0 #Change here!\n",
    "print(\"The value of that policy is : {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 245,
       "height": 248
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(22).astype(int)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}