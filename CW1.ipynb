{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RL Coursework 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Questions 1\n",
    "a. Write out your personalised trace. Please use exactly the same format as in the example above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "\n",
    "full_start_time = time.time()\n",
    "\n",
    "\n",
    "cid = 1349379\n",
    "cid = [int(x) for x in str(cid)]\n",
    "\n",
    "states = [np.mod((cid[t] + 2), 4) for t in range(len(cid))]\n",
    "rewards = [np.mod((cid[t]), 4) for t in range(len(cid))]\n",
    "\n",
    "trace = [None]*(len(states)+len(rewards))\n",
    "trace[::2] = states\n",
    "trace[1::2] = rewards\n",
    "\n",
    "print(\"The trace for the CID: {} is {}\".format(cid, trace))\n"
   ]
  },
  {
   "source": [
    "# Question 2\n",
    "\n",
    "a. State your personalised reward state, p, and γ (1 pts)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridElement():\n",
    "    def __init__(self, name=None, reward=None, location_coords=None, is_terminal=False, is_wall=False):\n",
    "        self.location = location_coords\n",
    "        self.reward = reward\n",
    "        self.is_terminal = is_terminal\n",
    "        self.is_wall = is_wall\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.is_wall:\n",
    "            return '[ ]'\n",
    "        return str(self.reward)\n",
    "    \n",
    "    def set_as_wall(self):\n",
    "        self.is_wall = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGridWorld():\n",
    "    def __init__(self, \n",
    "            absorbing_locs=None,\n",
    "            special_rewards=None,\n",
    "            p_transition=0.45,\n",
    "            obstacle_locs=[(1,1), (2,3), (2, 5), (3,1), (4,1), (4,2), (4,4)],\n",
    "            shape=(6,6)):\n",
    "        self.reward_state_rew = reward_state_rew\n",
    "        self.penalty_state_rew = penalty_state_rew\n",
    "\n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = shape\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = obstacle_locs\n",
    "        self.absorbing_locs = absorbing_locs\n",
    "\n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        self.special_rewards = special_rewards\n",
    "\n",
    "        # Starting location\n",
    "        self.starting_loc = (1,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = {\n",
    "            'North': 'N',\n",
    "            'East': 'E',\n",
    "            'South': 'S',\n",
    "            'West': 'W'}\n",
    "        self.action_names_list = ['N', 'E', 'S', 'W']\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        self.p_transition = p_transition\n",
    "\n",
    "        # Measuring convergence via delta's\n",
    "        self.delta_history = []\n",
    "        \n",
    "        ############################################\n",
    "\n",
    "\n",
    "        #### Internal State  ####\n",
    "        \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs, neighbors = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "\n",
    "        # Neighbors\n",
    "        self.neighbors = neighbors\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs)\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape)\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        # self.paint_maps()\n",
    "        ################################\n",
    "\n",
    "\n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    ########################\n",
    "\n",
    "\n",
    "    ####### Methods #########\n",
    "    ##########################\n",
    "\n",
    "\n",
    "    # DRAW\n",
    "\n",
    "    ############# PLOTTING AND DRAWING #############\n",
    "    def draw_learningcurve_repvars(self, all_curves, title_text='', var_labels=[], axislabels=('Episodes', 'Mean return'), new_fig=False, save=None):\n",
    "        \"\"\" Plot different curves on the same plot with repeats.\n",
    "        all_curve[var, rep, curve_values] \"\"\"\n",
    "        \n",
    "        if not new_fig:\n",
    "            plt.figure()\n",
    "        for var_num, var_curves in enumerate(all_curves):\n",
    "            if new_fig:\n",
    "                plt.figure()\n",
    "            mean_curve = np.average(var_curves, axis=0)\n",
    "            std_curve = np.std(var_curves, axis=0) \n",
    "\n",
    "            label = var_labels[var_num]\n",
    "            \n",
    "            plt.plot(mean_curve, label=label)\n",
    "            plt.fill_between(list(range(len(mean_curve))), (mean_curve - std_curve), (mean_curve + std_curve), alpha=0.6)\n",
    "        \n",
    "            plt.title(title_text)\n",
    "            plt.xlabel(axislabels[0])\n",
    "            plt.ylabel(axislabels[1])\n",
    "            if new_fig:\n",
    "                plt.legend(loc=\"upper left\")\n",
    "\n",
    "        if not new_fig:\n",
    "            plt.legend(loc=\"upper left\")\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_learning_curve_together(self, all_curves, title_text='', labels=[], repeats=False, new_fig=False, multiple_curves=False, save=None):\n",
    "        \"\"\" Plot multiple learning curves on same plot \"\"\"\n",
    "        plt.figure()\n",
    "        num_separate_curves = len(all_curves) if multiple_curves else 1\n",
    "        for c in range(num_separate_curves):\n",
    "            some_curves = all_curves[c] if num_separate_curves>1 else all_curves\n",
    "            for i, curves in enumerate(some_curves):\n",
    "                label = labels[i] if labels else None\n",
    "                # title_text = title_text+label if title_text else ''\n",
    "                self.draw_learning_curve(curves, title_text=title_text, label=label, new_fig=new_fig, repeats=repeats)\n",
    "        plt.title(title_text)\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Mean discounted rewards')\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_learning_curve(self, all_mean_rewards, title_text='', label='', new_fig=True, repeats=True, axislabels=('Episodes','Mean discounted rewards'), save=None):\n",
    "        \"\"\" Draw the rewards collected per episode \"\"\"\n",
    "        mean_mean_rewards = np.average(all_mean_rewards, axis=0) if repeats else all_mean_rewards \n",
    "        std_mean_rewards = np.std(all_mean_rewards, axis=0) if repeats else 0\n",
    "        if new_fig:\n",
    "            plt.figure()\n",
    "            plt.title(title_text)\n",
    "            plt.xlabel(axislabels[0])\n",
    "            plt.ylabel(axislabels[1])\n",
    "        plt.plot(mean_mean_rewards, label=label)\n",
    "        if repeats:\n",
    "            plt.fill_between(list(range(len(mean_mean_rewards))), (mean_mean_rewards - std_mean_rewards), (mean_mean_rewards + std_mean_rewards), alpha=0.6)\n",
    "        if new_fig:\n",
    "            if save:\n",
    "                plt.savefig(save)\n",
    "            # plt.show()\n",
    "\n",
    "    def draw_deterministic_policy(self, Policy, V=None, title='Policy grid', save=None):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(V)\n",
    "        plt.imshow(walls + absorbers + value_states + rewarders, cmap=cm.RdYlBu_r)\n",
    "        # plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                policy_text = 'Absorbing'\n",
    "                location = self.locs[state]\n",
    "                plt.text(location[1], location[0], policy_text, ha='center', va='center')\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action = np.argmax(Policy[state, :])\n",
    "            action = int(action)\n",
    "            action_arrow = arrows[action]\n",
    "\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "            plt.title(title)\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_stochastic_policy(self, Policy, V=None, title='Policy for Grid World', save=None):\n",
    "        # Draw a stochastic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure(figsize=(12,8))  # \n",
    "\n",
    "        walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(V)\n",
    "        plt.imshow(walls + absorbers + value_states + rewarders, cmap=cm.RdYlBu_r)  # Spectral_r coolwarm\n",
    "        # plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                policy_text = 'Absorbing'\n",
    "                location = self.locs[state]\n",
    "                plt.text(location[1], location[0], policy_text, ha='center', va='center')\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action = np.argmax(Policy[state, :])\n",
    "            action = int(action)\n",
    "            action_arrow = arrows[action]\n",
    "\n",
    "            location = self.locs[state]\n",
    "\n",
    "            # Show the probability for each action (first 4 digits)\n",
    "            Policy = np.around(Policy, decimals=4)\n",
    "            policy_text = ['{}\\n'.format(action_prob) for action_prob in Policy[state, :]]\n",
    "            policy_text = ''.join(policy_text)\n",
    "            policy_text = action_arrow + '\\n' + policy_text\n",
    "            plt.text(location[1], location[0], policy_text, ha='center', va='center')\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_stochastic_policy_grid(self, Policies, Values, titles, n_columns, n_lines):\n",
    "        # Draw grid of policies\n",
    "\n",
    "        plt.figure(figsize=(35,8))\n",
    "        vmin = np.min(Values)\n",
    "        vmax = np.max(Values)\n",
    "        for subplot in range(len(Policies)): # Go through all policies\n",
    "            ax = plt.subplot(n_columns, n_lines, subplot+1)         # Create a subplot for each policy\n",
    "            Policy = Policies[subplot]\n",
    "            V = Values[subplot]\n",
    "            for state, action in enumerate(Policy):\n",
    "                walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(V)\n",
    "                ax.imshow(walls + absorbers + value_states + rewarders, cmap=cm.RdYlBu_r, aspect='auto', vmin=vmin, vmax=vmax)  # Create the graph of the grid\n",
    "\n",
    "                if(self.absorbing[0,state]):\n",
    "                    continue\n",
    "                arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "                action = np.argmax(Policy[state, :])\n",
    "                action = int(action)\n",
    "                action_arrow = arrows[action]\n",
    "\n",
    "                location = self.locs[state]\n",
    "\n",
    "                # Show the probability for each action (first 4 digits)\n",
    "                Policy = np.around(Policy, decimals=4)\n",
    "                policy_text = ['{}\\n'.format(action_prob) for action_prob in Policy[state, :]]\n",
    "                policy_text = ''.join(policy_text)\n",
    "                policy_text = action_arrow + '\\n' + policy_text\n",
    "                plt.text(location[1], location[0], policy_text, ha='center', va='center')\n",
    "                plt.colorbar()\n",
    "            ax.title.set_text(titles[subplot]) # Set the title for the graoh given as argument\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "    def get_world_vars_plotting(self, V):\n",
    "        \"\"\" Helper var for making walls etc. look good \"\"\"\n",
    "        walls = self.show_as_nan(self.walls)\n",
    "        absorbers_idx = np.where(self.absorbers != 0)\n",
    "        absorbers_idx = zip(absorbers_idx[0], absorbers_idx[1])\n",
    "        rewarders = self.rewarders\n",
    "        absorbers = self.absorbers\n",
    "        for i, j in absorbers_idx:\n",
    "            rewarders[i, j] = 0\n",
    "            absorbers[i, j] = 0\n",
    "        value_states = self.get_value_state_grid(V)\n",
    "\n",
    "        return walls, absorbers, rewarders, value_states\n",
    "\n",
    "    def get_value_state_grid(self, V):\n",
    "        \"\"\" Convert V to GridWorld format \"\"\"\n",
    "        value_states = np.zeros(self.absorbers.shape)\n",
    "        if np.any(V):\n",
    "            for flat_idx in range(len(V)):\n",
    "                state_loc = self.state_to_loc(flat_idx, self.locs)\n",
    "                value_states[state_loc[0], state_loc[1]] = V[flat_idx]\n",
    "            # for i in range(value_states.shape[0]):\n",
    "            #     for j in range(value_states.shape[1]):\n",
    "            #         flat_idx = i*value_states.shape[1] + j\n",
    "            #         state_loc = self.state_to_loc(flat_idx)\n",
    "            #         value_states[i, j] = V[flat_idx]\n",
    "        return value_states\n",
    "\n",
    "    def show_as_nan(self, grid_elements):\n",
    "        # For plotting function, show the non-zero elements here as nan\n",
    "        grid_elements[np.nonzero(grid_elements)] = np.nan\n",
    "        return grid_elements\n",
    "\n",
    "    def draw_value(self, Value, title='Value for Grid World', save=None):\n",
    "        # Draw a policy value function\n",
    "        value_states = self.get_value_state_grid(Value)\n",
    "        \n",
    "        plt.figure()\n",
    "        walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(Value)\n",
    "        vmin=np.min(Value)\n",
    "        vmax=np.max(Value)\n",
    "        plt.imshow(walls +absorbers +rewarders +value_states, cmap=cm.RdYlBu_r, vmin=vmin, vmax=vmax)  # Create the graph of the grid (leave +self.rewarders)\n",
    "        for state, value in enumerate(Value):\n",
    "            if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
    "                continue\n",
    "            location = self.locs[state] # Compute the value location on graph\n",
    "            plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
    "    \n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_delta_history(self):\n",
    "        # Draw the progression of deltas with each epoch\n",
    "        fig = plt.figure()\n",
    "\n",
    "        plt.plot(self.delta_history)\n",
    "        plt.title('Value convergence')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Delta')\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_rewards(self, reward_episode_tracking, use_first_visit=True, save=None):\n",
    "        # Plot the mean total rewards per episode vs. episodes\n",
    "        fig = plt.figure()\n",
    "\n",
    "        visit_type = '(First-visit)' if use_first_visit else '(Every-visit)'\n",
    "\n",
    "        plt.plot(reward_episode_tracking)\n",
    "        plt.title('Mean rewards collected per episode {}'.format(visit_type))\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Mean Reward')\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_deterministic_policy_grid(self, Policy, Values, titles, n_columns, n_lines, save=None):\n",
    "        # Draw a grid of deterministic policy\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure(figsize=(12,12))\n",
    "        walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(Values[0])\n",
    "        vmin=np.min(Values)\n",
    "        vmax=np.max(Values)\n",
    "        im = plt.imshow(walls +absorbers +rewarders +value_states, cmap=cm.RdYlBu_r, vmin=vmin, vmax=vmax)\n",
    "        for subplot in range (len(Policy)): # Go through all policies\n",
    "            ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy\n",
    "            PCM=ax.get_children()[2] #get the mappable, the 1st and the 2nd are the x and y axes\n",
    "            Value = Values[subplot]\n",
    "            walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(Value)\n",
    "            ax.imshow(walls +absorbers +rewarders +value_states, cmap=cm.RdYlBu_r, vmin=vmin, vmax=vmax)\n",
    "            for state, action in enumerate(Policy[subplot]):\n",
    "                if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
    "                    continue\n",
    "                arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
    "                action = np.argmax(action)\n",
    "                action = int(action)\n",
    "                action_arrow = arrows[action] # Take the corresponding action\n",
    "                location = self.locs[state] # Compute its location on graph\n",
    "                plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
    "            ax.title.set_text(titles[subplot]) # Set the title for the graoh given as argument\n",
    "            plt.colorbar(im, ax=ax)\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "    def draw_value_grid(self, Value, titles, n_columns, n_lines, save=None):\n",
    "        # Draw a grid of value function\n",
    "        plt.figure(figsize=(12,12))\n",
    "        walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(Value[0])\n",
    "        vmin=np.min(Value)\n",
    "        vmax=np.max(Value)\n",
    "        im = plt.imshow(walls +absorbers +rewarders +value_states, cmap=cm.RdYlBu_r, vmin=vmin, vmax=vmax)\n",
    "        for subplot in range (len(Value)): # Go through all values\n",
    "            ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value\n",
    "            walls, absorbers, rewarders, value_states = self.get_world_vars_plotting(Value[subplot])\n",
    "            ax.imshow(walls +absorbers +rewarders +value_states, cmap=cm.RdYlBu_r, vmin=vmin, vmax=vmax)\n",
    "            for state, value in enumerate(Value[subplot]):\n",
    "                if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
    "                    continue\n",
    "                location = self.locs[state] # Compute the value location on graph\n",
    "                plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph\n",
    "            ax.title.set_text(titles[subplot]) # Set the title for the graoh given as argument\n",
    "            plt.colorbar(im, ax=ax)\n",
    "        if save:\n",
    "            plt.savefig(save)\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # DRAW\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        # Helper function to print the grid word used in __init__\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.title('Obstacles')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.title('Absorbing states')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.title('Reward states')\n",
    "        plt.show()\n",
    "\n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbors of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbors, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,self.action_size))\n",
    "\n",
    "        # Possible probabilities of transitions\n",
    "        self_prob = self.p_transition  # self.action_randomizing_array[effect]\n",
    "        neighbor_prob = (1 - self.p_transition) / (self.action_size - 1)\n",
    "        \n",
    "        # To fill T, we need to check if the current action leads us to the\n",
    "        # next state. \n",
    "        # For each prior state, choose an action, then assign probs to each post_state\n",
    "        for prior_state in range(S):\n",
    "            for action in range(self.action_size):\n",
    "                for effect in range(self.action_size):\n",
    "                    post_state = int(neighbors[prior_state, effect])\n",
    "\n",
    "                    # If this is the desired action\n",
    "                    if action == effect:\n",
    "                        T[post_state,prior_state,action] = self_prob\n",
    "                    else:\n",
    "                        T[post_state,prior_state,action] = neighbor_prob\n",
    "\n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,self.action_size))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "\n",
    "        return S, T,R,absorbing,locations, neighbors\n",
    "    \n",
    "\n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbor_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbors of each state, in terms of locations\n",
    "                    local_neighbors = [self.get_neighbor(loc,direction) for direction in list(self.action_names.values())]\n",
    "                    neighbor_locs.append(local_neighbors)\n",
    "                \n",
    "        # translate neighbor lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbors = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbor location\n",
    "                nloc = neighbor_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbor matrix\n",
    "                state_neighbors[state,direction] = nstate\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbors, absorbing \n",
    "\n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "    def state_to_loc(self, state, locs):\n",
    "        #convert given (valid) state number to its grid location\n",
    "        return locs[state]\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "            \n",
    "    def get_neighbor(self,loc,direction):\n",
    "        #Find the valid neighbors (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbor is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == self.action_names['North'] and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == self.action_names['East'] and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == self.action_names['South'] and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == self.action_names['West'] and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "\n",
    "    def states_to_direction(self, prior, post):\n",
    "        \"\"\" Return the direction needed to go from prior state to post state.\n",
    "        Aka the direction you are facing when going to post state from prior. \"\"\"\n",
    "\n",
    "        prior_loc = self.state_to_loc(prior, self.locs)\n",
    "        post_loc = self.state_to_loc(post, self.locs)\n",
    "        # print('prior_loc', prior_loc)\n",
    "        # print('post_loc', post_loc)\n",
    "\n",
    "        if prior_loc[0] < post_loc[0]:\n",
    "            return self.action_names_list.index(self.action_names['North'])\n",
    "        if prior_loc[1] < post_loc[1]:\n",
    "            return self.action_names_list.index(self.action_names['East'])\n",
    "        if prior_loc[0] > post_loc[0]:\n",
    "            return self.action_names_list.index(self.action_names['South'])\n",
    "        if prior_loc[1] > post_loc[1]:\n",
    "            return self.action_names_list.index(self.action_names['West'])\n",
    "        else:\n",
    "            return np.random.randint(0, self.action_size)\n",
    "\n",
    "    # def convert_index_to_loc(self, idx, row_num, col_num):\n",
    "    #     row_idx = idx // row_num\n",
    "    #     col_idx = idx % col_num\n",
    "    #     return (row_idx, col_idx)\n",
    "\n",
    "        \n",
    "################## End world class #########################         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## DP class #########################         \n",
    "\n",
    "class DP_Policy(NewGridWorld):\n",
    "    \n",
    "    def __init__(self, \n",
    "            absorbing_locs=None,\n",
    "            special_rewards=None,\n",
    "            p_transition=0.45\n",
    "            ):\n",
    "        super(DP_Policy, self).__init__(absorbing_locs, special_rewards, p_transition)\n",
    "\n",
    "    ####### Methods #########\n",
    "    def value_iteration(self, discount = 0.9, threshold = 0.0001):\n",
    "        ## Slide 144 of the lecture notes for the algorithm ##\n",
    "        \n",
    "        # Transition and reward matrices, both are 3d tensors, c.f. internal state\n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        # Initialisation\n",
    "        epochs = 0\n",
    "        delta = threshold # Setting value of delta to go through the first breaking condition\n",
    "        V = np.zeros(self.state_size) # Initialise values at 0 for each state\n",
    "\n",
    "        while delta >= threshold:\n",
    "            epochs += 1 # Increment the epoch\n",
    "            delta = 0 # Reinitialise delta value\n",
    "\n",
    "            # For each state\n",
    "            for state_idx in range(self.state_size):\n",
    "\n",
    "                # If not an absorbing state\n",
    "                if not(self.absorbing[0,state_idx]):\n",
    "                  \n",
    "                    # Store the previous value for that state\n",
    "                    old_V = V[state_idx] \n",
    "\n",
    "                    # Compute Q value\n",
    "                    Q = np.zeros(4) # Initialise with value 0\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        current_return = R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime]\n",
    "                        Q += T[state_idx_prime,state_idx,:] * current_return\n",
    "                \n",
    "                    # Set the new value to the maximum of Q\n",
    "                    V[state_idx]= np.max(Q) \n",
    "\n",
    "                    # Compute the new delta\n",
    "                    delta = max(delta, np.abs(old_V - V[state_idx]))\n",
    "                    # delta = max(abs(Vnew-V))\n",
    "            \n",
    "        # When the loop is finished, fill in the optimal policy\n",
    "        optimal_policy = np.zeros((self.state_size, self.action_size)) # Initialisation\n",
    "\n",
    "        # For each state\n",
    "        for state_idx in range(self.state_size):\n",
    "            # If not an absorbing state\n",
    "            if not(self.absorbing[0,state_idx]):\n",
    "\n",
    "                # Compute Q value\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                \n",
    "                # The action that maximises the Q value gets probability 1\n",
    "                optimal_policy[state_idx, np.argmax(Q)] = 1 \n",
    "\n",
    "        optimal_V = V\n",
    "\n",
    "        return optimal_policy, optimal_V, epochs\n",
    "\n",
    "\n",
    "    def policy_iteration(self, policy, discount=0.9, threshold = 0.0001):\n",
    "        \"\"\" Policy Improvement / Iterative policy to get optimal value and policy. Full sweep. \"\"\"\n",
    "        epochs = [0]\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        returns = [0]\n",
    "\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "\n",
    "        # Prepare plot\n",
    "        # fig = plt.figure()\n",
    "        # ax = fig.add_subplot(111)\n",
    "        # Ln = ax.plot(values)\n",
    "        # ax.plot(returns)\n",
    "        # ax.set_xlim([0,20])\n",
    "        # plt.ion()\n",
    "        # plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        # plt.show()  \n",
    "\n",
    "        is_policy_stable = False\n",
    "        while (not is_policy_stable) and (epochs[-1] < 2000):\n",
    "            is_policy_stable = True\n",
    "\n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "\n",
    "            epochs.append(epochs[-1] + epochs_eval)\n",
    "            returns.append(np.sum(V))\n",
    "            # print('Iteration Epochs:', epochs[-1])\n",
    "\n",
    "            # Policy iteration\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If not an absorbing state\n",
    "                if not(self.absorbing[0,state_idx]):\n",
    "\n",
    "                    # Store the old action\n",
    "                    old_action = np.argmax(policy[state_idx,:])\n",
    "\n",
    "                    # Compute Q value\n",
    "                    Q = np.zeros(4) # Initialise with value 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        current_return = R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime]\n",
    "                        Q += T[state_idx_prime,state_idx,:] * current_return\n",
    "\n",
    "                    # Compute corresponding policy\n",
    "                    new_policy = np.zeros(4)\n",
    "                    new_policy[np.argmax(Q)] = 1  # The action that maximises the Q value gets probability 1\n",
    "                    policy[state_idx] = new_policy\n",
    "                    \n",
    "                    if old_action != np.argmax(policy[state_idx]):\n",
    "                        is_policy_stable = False\n",
    "\n",
    "            ### PLOT V progress ###\n",
    "            # ax.plot(returns, epochs)\n",
    "            # self.draw_value(V)\n",
    "            # Ln.set_ydata(values)\n",
    "            # Ln.set_xdata(epochs)\n",
    "            # plt.pause(1)\n",
    "\n",
    "        return policy, V, epochs\n",
    "\n",
    "\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        # Measuring convergence via delta's\n",
    "        self.delta_history = []\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "\n",
    "        epoch = 0        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta = max(abs(Vnew-V))\n",
    "            self.delta_history.append(delta)\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "\n",
    "    ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The value of p is 0.45 and the value of gamma is 0.55\nThe reward state is s_2\n"
     ]
    }
   ],
   "source": [
    "cid_nums = 379\n",
    "x = int(str(cid_nums)[0])\n",
    "y = int(str(cid_nums)[1])\n",
    "z = int(str(cid_nums)[2])\n",
    "\n",
    "p = 0.25 + 0.5 * (x+1)/10\n",
    "gamma = 0.2 + 0.5 * y/10\n",
    "j = np.mod((z+1), 3) + 1\n",
    "threshold = 0.0001\n",
    "\n",
    "print(\"The value of p is {} and the value of gamma is {}\".format(p, gamma))\n",
    "print(\"The reward state is s_{}\".format(j))"
   ]
  },
  {
   "source": [
    "b. Dynamic Programming (with full world knowledge) (24 pts)\n",
    "\n",
    "1. Compute the optimal value function and the optimal policy using Dynamic Programming.\n",
    "Briefly state how you solved the problem, including any parameters that you set or assumptions you made."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4. Briefly discuss how the value of your γ and p have influenced the optimal value function\n",
    "and optimal policy in your personal Grid World. In particular, you may investigate the\n",
    "effect of having a value of p < 0.25, p = 0.25 or p > 0.25, and similarly γ < 0.5 or γ > 0.5."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GridWorld\n",
    "reward_state_rew = ((1, 2), 10)     # State 2\n",
    "penalty_state_rew = ((4, 3), -100)  # State 11\n",
    "\n",
    "absorbing_locs = [reward_state_rew[0], penalty_state_rew[0]]\n",
    "special_rewards = [reward_state_rew[1], penalty_state_rew[1]]\n",
    "\n",
    "# p = 1\n",
    "\n",
    "dp_world = DP_Policy(absorbing_locs=absorbing_locs, special_rewards=special_rewards, p_transition=p)\n",
    "\n",
    "# Compute optimal value function\n",
    "Policy = np.zeros((dp_world.state_size, dp_world.action_size))\n",
    "\n",
    "policy_opt_DPpit, V_optimal_DPpit, epochs_DPpit = dp_world.policy_iteration(Policy, discount=gamma, threshold=threshold)\n",
    "policy_opt_DPvit, V_optimal_DPvit, epochs_DPvit = dp_world.value_iteration(gamma, threshold)\n",
    "\n",
    "# dp_world.draw_stochastic_policy(policy_opt_DPpit, V_optimal_DPpit, title=r'DP: Policy iteration, $\\gamma$={} threshold={}'.format(gamma, threshold), save='DP_pit_policy-world.png')\n",
    "# dp_world.draw_stochastic_policy(policy_opt_DPvit, V_optimal_DPvit, title=r'DP: Value iteration, $\\gamma$={} threshold={}'.format(gamma, threshold), save='DP_vit_policy-world.png')\n",
    "\n",
    "if np.all(policy_opt_DPpit == policy_opt_DPvit):\n",
    "    print(\"The policy iteration and value iteration both reached the same optimal policy.\")\n",
    "\n",
    "print('\\n2.b.4 Observe change of p and gamma')\n",
    "start_time = time.time()\n",
    "p_collection = [0.1, 0.25, 0.5] \n",
    "gamma_collection = [0.3, 0.5, 0.7]\n",
    "Ps = p_collection * len(gamma_collection)\n",
    "gammas = np.sort(gamma_collection * len(p_collection))\n",
    "policies_dp = []\n",
    "values_dp = []\n",
    "titles = []\n",
    "for p_, gamma_ in zip(Ps, gammas):\n",
    "    dp_world = DP_Policy(absorbing_locs=absorbing_locs, special_rewards=special_rewards, p_transition=p_)\n",
    "    policy_opt_DPvit, V_optimal_DPvit, epochs_DPvit = dp_world.value_iteration(gamma_, threshold)\n",
    "    policies_dp.append(policy_opt_DPvit)\n",
    "    values_dp.append(V_optimal_DPvit)\n",
    "    titles.append(r'p={} $\\gamma$={}'.format(p_, gamma_))\n",
    "# dp_world.draw_deterministic_policy_grid(policies_dp, values_dp, titles, n_columns=len(p_collection), n_lines=len(gamma_collection), save='DP_policy_grid.png')\n",
    "dp_world.draw_value_grid(values_dp, titles=titles, n_columns=len(p_collection), n_lines=len(gamma_collection), save='DP_value_grid.png')\n",
    "\n",
    "print('2.b.4 took {}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration Epochs: 1\n",
      "Iteration Epochs: 23\n",
      "Iteration Epochs: 42\n",
      "Iteration Epochs: 62\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-11-06T18:26:46.127738</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 392.14375 277.314375 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \nL 384.94375 22.318125 \nL 50.14375 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m48af58153e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(57.410369 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"105.409779\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2.5 -->\n      <g transform=\"translate(97.458216 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"145.457626\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5.0 -->\n      <g transform=\"translate(137.506063 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.505472\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7.5 -->\n      <g transform=\"translate(177.55391 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.553319\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10.0 -->\n      <g transform=\"translate(214.420507 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.601166\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12.5 -->\n      <g transform=\"translate(254.468354 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"305.649013\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15.0 -->\n      <g transform=\"translate(294.516201 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"345.69686\" xlink:href=\"#m48af58153e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17.5 -->\n      <g transform=\"translate(334.564048 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- Epoch -->\n     <g transform=\"translate(202.232813 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4ce25e9562\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"229.882069\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.0 -->\n      <g transform=\"translate(27.240625 233.681288)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"204.07524\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.5 -->\n      <g transform=\"translate(27.240625 207.874459)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"178.268412\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 5.0 -->\n      <g transform=\"translate(27.240625 182.06763)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"152.461583\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 7.5 -->\n      <g transform=\"translate(27.240625 156.260802)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"126.654754\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 10.0 -->\n      <g transform=\"translate(20.878125 130.453973)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"100.847926\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 12.5 -->\n      <g transform=\"translate(20.878125 104.647144)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"75.041097\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 15.0 -->\n      <g transform=\"translate(20.878125 78.840316)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"49.234268\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 17.5 -->\n      <g transform=\"translate(20.878125 53.033487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ce25e9562\" y=\"23.42744\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 20.0 -->\n      <g transform=\"translate(20.878125 27.226658)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- Delta -->\n     <g transform=\"translate(14.798438 144.377969)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"138.525391\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"166.308594\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"205.517578\" xlink:href=\"#DejaVuSans-97\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p2970380f22)\" d=\"M 65.361932 32.201761 \nL 81.381071 192.147324 \nL 97.400209 204.306482 \nL 113.419348 218.781905 \nL 129.438487 223.887465 \nL 145.457626 226.849906 \nL 161.476764 228.28728 \nL 177.495903 229.030716 \nL 193.515042 229.296696 \nL 209.534181 229.50452 \nL 225.553319 229.620643 \nL 241.572458 229.708434 \nL 257.591597 229.763692 \nL 273.610736 229.802572 \nL 289.629874 229.828172 \nL 305.649013 229.845729 \nL 321.668152 229.857483 \nL 337.687291 229.865469 \nL 353.706429 229.870847 \nL 369.725568 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 239.758125 \nL 50.14375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 239.758125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Value convergence -->\n    <g transform=\"translate(160.4875 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-86\"/>\n     <use x=\"60.658203\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"121.9375\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"149.720703\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"213.099609\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"274.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"306.410156\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"361.390625\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"422.572266\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"485.951172\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"545.130859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"606.654297\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"646.017578\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"709.494141\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"771.017578\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"834.396484\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"889.376953\" xlink:href=\"#DejaVuSans-101\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2970380f22\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlq0lEQVR4nO3de3xcdZ3/8dc7SZv0kkBLM0NLgYoiCiwixoKIrohcZa2iPyzrIghsFxZccW+y629dfroXdHfd3yIIVkFAEWEV2P4UBURXQAQpbLkWaEGQltKkFGhaoLd8fn+cM+F0OpOkSWYmmXk/H495zDnf7/ec+eTk8sk53/M9X0UEZmZmxZpqHYCZmY1NThBmZlaSE4SZmZXkBGFmZiU5QZiZWUlOEGZmVpIThI17kkLSm2odh1m9cYKwmpP0U0lfLFE+T9LzklpqEZdZo3OCsLHgSuCPJKmo/GTg6ojYUoOYxjQnTasGJwgbC24EdgHeUyiQNA04HrhK0lxJv5b0kqRVki6SNLHUjiT9t6QzMuunSrozs/4WSbdKWivpcUknlgtK0nRJ35b0nKQXJd2YqftjScvT/SySNCtTF5LOlLQsjfliJVrT9f0zbTslvSopl64fL2lJ2u4uSQdk2j4t6XOSHgQ2SGqR9ElJz0h6QdLfpW0+kLZvknSepCfT+uskTU/r5qRxniLpd5LWSPp85rOaJf1tum2vpPsk7b6jx9DGNycIq7mIeBW4DvhkpvhE4LGIeADYCnwWmAG8CzgC+NMd/RxJU4Bbge8BOWA+8HVJ+5bZ5DvAZGC/tP2/p/t5P/DPaYwzgWeA7xdtezzwTuCAtN3REbERuB44qejr/GVEdEt6O3A58CckCfMbwCJJrZn2JwEfBHYG3gx8HfhEGsdOwG6Ztp8GPgz8PjALeBG4uCjOw4B9SI7pFyS9NS3/8/SzjgM6gNOAV4ZxDG08iwi//Kr5i+QP1UtAW7r+K+CzZdqeC9yQWQ/gTenyfwNnZOpOBe5Mlz8O3FG0r28Af1/iM2YCfcC0EnWXAV/JrE8FNgNzMvEclqm/DjgvXf4A8GSm7lfAJ9PlS4AvFX3W48Dvp8tPA6dl6r4AXJNZnwxsAj6Qri8Fjij6mjYDLcCcNM7ZmfrfAPMznzuvxNc+5GPo1/h/+TqmjQkRcaekNcCHJd0LzAVOAJD0ZuCrQBfJH8EW4L5hfMyewMGSXsqUtZCcKRTbHVgbES+WqJsF3J+Jfb2kF0j+e386LX4+0/4VkiQC8AtgsqSDgdXAgcANmfhOkfTpzLYT088reLYojv71iHgljaNgT+AGSX2Zsq1APrNeLs7dgSfZ3o4cQxvnnCBsLLmK5DLTPsDNEbE6Lb8E+B/gpIjolXQu8LEy+9hAkkQKds0sP0tyOefIIcTyLDBd0s4R8VJR3XMkfyiB/ktXuwArB9tpRGyVdB3J5ZvVwI8iojfzmf8YEf840C4yy6tIjlUhjklpHNmv4bSI+FXxTiTNGSTUZ4E3Ag+XKB/qMbRxzn0QNpZcRXIJ5o9J7mwqaAfWAeslvQU4a4B9LAFOkDQ5HRtxeqbuR8CbJZ0saUL6emfmunu/iFgF/ITk+vq0tO170+prgE9JOjDtH/gn4J6IeHqIX+f3SC7VfCJdLvgmcKakg9NO7SmSPiipvcx+fgD8gaRD007784HsnWCXAv8oaU/o7xCfN8QYvwV8SdLeaSwHSNqFHTiGNv45QdiYkf6BvQuYAizKVP0l8IdAL8kf0WsH2M2/k1yHX02SZK7O7L8XOIqkY/U5kssrXwZat98NkNxmuxl4DOgm6fsgIn4G/B3wQ5L/4t+Y7nNIIuIekjOdWSRJqFC+mCQ5XkTSobycpA+l3H4eIemI/n4ax/o0zo1pk/8gOY63SOoF7gYOHmKYXyXpO7mFJDlfBkwaxjG0cUwRnjDIrB5ImkrS0b93RPy2xuFYHfAZhNk4JukP0stpU4B/BR7i9Y5ysxFxgjAb3+aRXOp5Dtib5DZVXxawUVGxBCFpd0m/kPSopEckfSYtn56OwlyWvk8rs/0paZtlkk6pVJxm41lEnBERO0fEThFxREQ8XuuYrH5UrA9C0kxgZkTcn96FcR/JqM5TSe4vv0DSeSQDkT5XtO10YDHJfe+RbvuOMvekm5lZBVRsHER6m+CqdLlX0lKSgUTzgPelza4kGfn6uaLNjwZujYi1AJJuBY4hub2wrBkzZsScOXNG5wswM2sA991335qI6CxVV5WBcumgnLcD9wD5NHlAcotcvsQmu7HtiNEVbPuMmey+FwALAPbYYw8WL148SlGbmdU/Sc+Uq6t4J3V6690PgXMjYl22Lu1MG9E1rohYGBFdEdHV2VkyCZqZ2TBUNEFImkCSHK6OiOvT4tVp/0Shn6K7xKYrSZ4FUzCbITzGwMzMRk8l72ISyejLpRHx1UzVIqBwV9IpwH+V2Pxm4Kj0EQfTSEZu3lypWM3MbHuVPIN4N8mjCt6vZAKUJZKOAy4AjpS0jOS5OxcASOqS9C2AtHP6S8C96euLhQ5rMzOrjrp61EZXV1e4k9rMbOgk3RcRXaXqPJLazMxKcoIwM7OSGj5BRARfu20Zv3yip9ahmJmNKQ2fICSx8I6n+MVjpe62NTNrXA2fIADyHW2sXvdarcMwMxtTnCCAXHurE4SZWREnCJIziO7ejYM3NDNrIE4QQK6jle51G6mnMSFmZiPlBAHk2tvYtLWPl17ZXOtQzMzGDCcIIN/RCuDLTGZmGU4QJH0QgDuqzcwynCBI7mICn0GYmWU5QZD0QYDPIMzMspwggEkTm+loa6HbCcLMrJ8TRCrnsRBmZttwgkjlOzya2swsywkilWtvY/U6n0GYmRW0VGrHki4Hjge6I2L/tOxaYJ+0yc7ASxFxYIltnwZ6ga3AlnKzHY2mXEcrPb3JaOpkOm0zs8ZWsQQBXAFcBFxVKIiIjxeWJf0b8PIA2x8eEWsqFl2RfGY09bQpE6v1sWZmY1bFLjFFxO3A2lJ1Sv5FPxG4plKfv6Ny6Wjq1b3uhzAzg9r1QbwHWB0Ry8rUB3CLpPskLRhoR5IWSFosaXFPz/BnhSuMpu52P4SZGVC7BHESA589HBYRBwHHAmdLem+5hhGxMCK6IqKrs7Nz2AHlPVjOzGwbVU8QklqAE4Bry7WJiJXpezdwAzC30nHl/MA+M7Nt1OIM4gPAYxGxolSlpCmS2gvLwFHAw5UOqm2CR1ObmWVVLEFIugb4NbCPpBWSTk+r5lN0eUnSLEk3pat54E5JDwC/AX4cET+tVJxZydzUPoMwM4MK3uYaESeVKT+1RNlzwHHp8lPA2yoV10ByHa10+y4mMzPAI6m3kfdoajOzfk4QGZ2Z0dRmZo3OCSIj77mpzcz6OUFk9E896n4IMzMniKz+x224H8LMzAkiqzCa2mMhzMycILbh0dRmZq9zgsgojKb285jMzJwgtpPvaPMTXc3McILYTr6jzXcxmZnhBLGdXHurzyDMzHCC2E6uo43u3tc8mtrMGp4TRJF8RyubtwYvejS1mTU4J4giucJYCPdDmFmDc4IokvdoajMzwAliOznPTW1mBjhBbKcwmrrHo6nNrMFVcsrRyyV1S3o4U3a+pJWSlqSv48pse4ykxyUtl3RepWIspW1CMztNmuAzCDNreJU8g7gCOKZE+b9HxIHp66biSknNwMXAscC+wEmS9q1gnNvJtbc6QZhZw6tYgoiI24G1w9h0LrA8Ip6KiE3A94F5oxrcIPIdbX5gn5k1vFr0QZwj6cH0EtS0EvW7Ac9m1lekZSVJWiBpsaTFPT09oxJgrsOjqc3Mqp0gLgHeCBwIrAL+baQ7jIiFEdEVEV2dnZ0j3R2Q3Mnk0dRm1uiqmiAiYnVEbI2IPuCbJJeTiq0Eds+sz07Lqsajqc3MqpwgJM3MrH4EeLhEs3uBvSW9QdJEYD6wqBrxFfTPTe2OajNrYJW8zfUa4NfAPpJWSDod+IqkhyQ9CBwOfDZtO0vSTQARsQU4B7gZWApcFxGPVCrOUnLtnlnOzKylUjuOiJNKFF9Wpu1zwHGZ9ZuA7W6BrRafQZiZeSR1SZ2FMwgnCDNrYE4QJRRGU/sSk5k1MieIMvIdHk1tZo3NCaKMXHubH/ltZg3NCaKMXEern+hqZg3NCaKMfDo3dV+fR1ObWWNygigj114YTb2p1qGYmdWEE0QZhbEQvpPJzBqVE0QZr89N7TuZzKwxOUGUUZib2o/9NrNG5QRRRv9o6l6fQZhZY3KCKOP1ual9BmFmjckJYgD5jlafQZhZw3KCGEC+w6OpzaxxOUEMoLO91U90NbOG5QQxgHxHGz3rN3o0tZk1JCeIAeQ9mtrMGlglpxy9XFK3pIczZf8i6TFJD0q6QdLOZbZ9Op2adImkxZWKcTC5/pnl3A9hZo2nkmcQVwDHFJXdCuwfEQcATwB/M8D2h0fEgRHRVaH4BlUYTe07mcysEVUsQUTE7cDaorJbImJLuno3MLtSnz8aPJrazBpZLfsgTgN+UqYugFsk3SdpwUA7kbRA0mJJi3t6ekY1wMJoaj+PycwaUU0ShKTPA1uAq8s0OSwiDgKOBc6W9N5y+4qIhRHRFRFdnZ2doxpn24Rmdp7suanNrDFVPUFIOhU4HvhERJS8fzQiVqbv3cANwNyqBVgk1+65qc2sMVU1QUg6Bvhr4EMR8UqZNlMktReWgaOAh0u1rYZkZjmfQZhZ46nkba7XAL8G9pG0QtLpwEVAO3BregvrpWnbWZJuSjfNA3dKegD4DfDjiPhppeIcTK69zaOpzawhtVRqxxFxUoniy8q0fQ44Ll1+CnhbpeLaUbmOVrp7k9HUTU2qdThmZlXjkdSDyLe3sqXPo6nNrPE4QQwi79HUZtagnCAGkSvMTe3R1GbWYJwgBlEYTd3jMwgzazBOEIPoP4PwnUxm1mCcIAbR2pKMpvYlJjNrNE4QQ5Bvb/MD+8ys4ThBDEGuo5XVHk1tZg3GCWIIPJrazBqRE8QQ5Dta6en13NRm1licIIYgl46mXuvR1GbWQJwghqAwmtod1WbWSJwghiBXeNyGb3U1swbiBDEEuXTqUXdUm1kjGdLjviXtDfwzsC/QViiPiL0qFNeYUhhN7UtMZtZIhnoG8W3gEpJ5pA8HrgK+W6mgxprWlmameTS1mTWYoSaISRFxG6CIeCYizgc+WLmwxp5ce5sf+W1mDWWoCWKjpCZgmaRzJH0EmDrYRpIul9Qt6eFM2XRJt0palr5PK7PtKWmbZZJOGWKcFVOYWc7MrFEMNUF8BpgM/BnwDuCPgE8OYbsrgGOKys4DbouIvYHb0vVtSJoO/D1wMDAX+PtyiaRa8h0eTW1mjWWoCWJORKyPiBUR8amI+Ciwx2AbRcTtwNqi4nnAlenylcCHS2x6NHBrRKyNiBeBW9k+0VRVrt2jqc2ssQw1QfzNEMuGIh8Rq9Ll54F8iTa7Ac9m1lekZduRtEDSYkmLe3p6hhnS4PIdbR5NbWYNZcDbXCUdCxwH7CbpwkxVB8kdTSMSESFpRP+SR8RCYCFAV1dXxf69L4yFWL3uNWZMba3Ux5iZjRmDnUE8B9wHvJa+F16LSC4DDcdqSTMB0vfuEm1WArtn1menZTWT8+M2zKzBDHgGEREPAA9I+m5EjPiMIbUIOAW4IH3/rxJtbgb+KdMxfRTDv6Q1KvKFwXIeC2FmDWKwS0wPAZEub1cfEQcMsv01wPuAGZJWkNyZdAFwnaTTgWeAE9O2XcCZEXFGRKyV9CXg3nRXX4yI4s7uqursv8TkMwgzawyDPWrj+JHsPCJOKlN1RIm2i4EzMuuXA5eP5PNHU2E0tc8gzKxRDHaJ6ZnCsqQ9gb0j4meSJg22bT3Kd3g0tZk1jiHd5irpj4EfAN9Ii2YDN1YopjGrs73Vg+XMrGEMdRzE2cC7gXUAEbEMyFUqqLEq39Hmx22YWcMY8rOYIqJ/hJikFtLO60aST5/H5NHUZtYIhpogfinpb4FJko4E/hP4f5ULa2zKtbextS94YYNHU5tZ/RtqgjgP6AEeAv4EuAn435UKaqzyWAgzayRDuhMpIvok3QjcGBGVe+DRGNfZ/vpo6v1m1TgYM7MKG/AMQonzJa0BHgcel9Qj6QvVCW9sKZxBrPadTGbWAAa7xPRZkruX3hkR0yNiOskcDe+W9NmKRzfGFEZT+04mM2sEgyWIk4GTIuK3hYKIeIqhTxhUV/rnpvYZhJk1gMESxISIWFNcmPZDTKhMSGObx0KYWaMYLEEMdD9nQ97rmfPUo2bWIAa7i+ltktaVKBfQVoF4xrxceytPPN9b6zDMzCpusIf1NVcrkPEi39FKz/pkNHVT0/aPQDczqxdDHShnqXyHR1ObWWNwgthB2bmpzczqWdUThKR9JC3JvNZJOreozfskvZxpM2YG5hXmpu7xnUxmVueqPulPRDwOHAggqRlYCdxQoukdETGiGe0qIZ8mCJ9BmFm9q/UlpiOAJ7Mz1411nVM9N7WZNYZaJ4j5wDVl6t4l6QFJP5G0XzWDGsjEliamT5noJ7qaWd2rWYKQNBH4EMncEsXuB/aMiLcBX2OA6U0lLZC0WNLinp7qPGg2197qMwgzq3u1PIM4Frg/IlYXV0TEuohYny7fBEyQNKPUTiJiYUR0RURXZ2dnZSNO5TrafAZhZnWvlgniJMpcXpK0qySly3NJ4nyhirENKN/eSrfPIMyszlX9LiYASVOAI0lmpyuUnQkQEZcCHwPOkrQFeBWYHxFjZiLoXDqaemtf0OzR1GZWp2qSICJiA7BLUdmlmeWLgIuqHddQFUZTr92wqX+OCDOzelPru5jGpVy7x0KYWf1zghiGXEdhZjknCDOrX04Qw1AYTe2OajOrZ04Qw+DR1GbWCJwghqEwmnq1LzGZWR1zghimnMdCmFmdc4IYJo+mNrN65wQxTPn2Vt/mamZ1zQlimPIdbaxZv4mtfWNmgLeZ2ahyghimXEdrOje1+yHMrD45QQxTYTS1O6rNrF45QQxT3qOpzazOOUEMU65/bmqfQZhZfXKCGKbCaGpfYjKzeuUEMUwTW5rYxaOpzayOOUGMQGd7K90eC2FmdcoJYgTyHW109/oSk5nVp5olCElPS3pI0hJJi0vUS9KFkpZLelDSQbWIcyA5j6Y2szpWkylHMw6PiDVl6o4F9k5fBwOXpO9jRr6jjZ5ez01tZvVpLF9imgdcFYm7gZ0lzax1UFn5jlb6Ao+mNrO6VMsEEcAtku6TtKBE/W7As5n1FWnZNiQtkLRY0uKenp4KhVpap0dTm1kdq2WCOCwiDiK5lHS2pPcOZycRsTAiuiKiq7Ozc3QjHERhNLX7IcysHtUsQUTEyvS9G7gBmFvUZCWwe2Z9dlo2ZvTPTe07mcysDtUkQUiaIqm9sAwcBTxc1GwR8Mn0bqZDgJcjYlWVQx3QjKk+gzCz+lWru5jywA2SCjF8LyJ+KulMgIi4FLgJOA5YDrwCfKpGsZZVGE3tMwgzq0c1SRAR8RTwthLll2aWAzi7mnENR66jzaOpzawujeXbXMeFZLCczyDMrP44QYxQvqPVc0KYWV1yghihXPvro6nNzOqJE8QI9Y+mXu/LTGZWX5wgRijnsRBmVqecIEYo1+6xEGZWn5wgRijvuanNrE45QYxQZ3oG8eyLr9Q4EjOz0eUEMUITmpt4x57T+MYvn+TiXyynz3czmVmdcIIYBVeeNpcPHjCLf7n5cU678l5e3LCp1iGZmY2YE8QomNrawoXzD+RLH96fu5a/wAcvvIP7f/dircMyMxsRJ4hRIomTD9mTH551KM3N4sRLf81ld/6W5JFSZmbjjxPEKPu92Tvxo3Pew+FvyfGlHz3KWd+9n3Wvba51WGZmO8wJogJ2mjyBhSe/g//9wbfys6WrOf7CO3l45cu1DsvMbIc4QVSIJM54z158f8EhbNrSxwmX3MXV9zzjS05mNm44QVRY15zp/PjPDuOQvXbh8zc8zGevXcKGjVtqHZaZ2aCcIKpgl6mtXHHqO/mLI9/MogeeY97Fv+KJ1b21DsvMbEBVTxCSdpf0C0mPSnpE0mdKtHmfpJclLUlfX6h2nKOtqUl8+oi9+e7pB/PSK5uZd9GvuP7+FbUOy8ysrFqcQWwB/iIi9gUOAc6WtG+JdndExIHp64vVDbFyDn3TDG76s8M4YPZO/Pl1D3DeDx/ktc1bax2Wmdl2qp4gImJVRNyfLvcCS4Hdqh1HLeU62rj6jIM5+/A38v17n+UjX7+LZ17YUOuwzMy2UdM+CElzgLcD95SofpekByT9RNJ+A+xjgaTFkhb39PRUKtRR19LcxF8d/Ra+feo7WfXyq5zw9bt4cMVLtQ7LzKxfzRKEpKnAD4FzI2JdUfX9wJ4R8Tbga8CN5fYTEQsjoisiujo7OysWb6Uc/pYc1591KJMmNjN/4d3c/sT4SXJmVt9qkiAkTSBJDldHxPXF9RGxLiLWp8s3ARMkzahymFWzV+dUrj/rUPbcZQqnXXEvN/7PylqHZGZWk7uYBFwGLI2Ir5Zps2vaDklzSeJ8oXpRVl+uo41r/+QQuuZM49xrl/CtO56qdUhm1uBaavCZ7wZOBh6StCQt+1tgD4CIuBT4GHCWpC3Aq8D8aIAhyB1tE7jytLl89tol/MOPl9LTu5HPHfMWmppU69DMrAFVPUFExJ3AgH/xIuIi4KLqRDS2tLY087WTDmLG1Ef4xu1P0dO7kS9/7AAmNHtMo5lVVy3OIGwQzU3i/3xoP3LtrfzrLU/wwoZNfP0TBzGl1d8uM6se/1s6RkninPfvzQUn/B53LOvhD791D2s9U52ZVZETxBg3f+4efOPkLh5btY6PXXIXz659pdYhmVmDcIIYB47cN8/VZxzMmvUb+egld7F0VfGwETOz0ecEMU50zZnOD846lOamZDrTu5+q67t+zWwMcIIYR96cb+eHZx1Kfqc2Pnn5b/jJQ6tqHZKZ1TEniHFm1s6T+MGZ72L/WR386ffu5zt3P1PrkMysTjlBjEM7T57I1Wccwvv3yfF3Nz7MV299wlOZmtmoc4IYpyZNbOYbJ7+DE7tmc+Fty/joJXdx2Z2/ZcWLvsvJzEaH6uk/z66urli8eHGtw6iqiOCKu57m2nuf5bHnk2lM99+tg6P33ZVj9t+VN+Wmkj7WysxsO5Lui4iuknVOEPXj6TUbuPmR57n5kee5/3cvAbDXjCkcvf+uHLPfrhwweycnCzPbhhNEA1q97jVueeR5bn5kNb9+6gW29gUzd2rj6P125aj98sydM50WP9/JrOE5QTS4l17ZxG1Lu/npI89z+xM9bNzSx7TJE/jAW/Mcs/+uvPtNM2ib0FzrMM2sBpwgrN8rm7bwy8d7+Okjz/Pzpd30btzClInNHLTnNPad1cG+M5PXG2ZM8RmGWQNwgrCSNm3p464n13DLo6tZ8ruXWNbdy+atyc9Da0sTb863s+/MDt46s523zuzgrbM66GibUOOozWw0OUHYkGza0seTPetZumodjz63jqXPr2Ppqt5tniI7e9qkNGkkr/1mdTB72iR3fpuNUwMlCE8wYP0mtjT1/+E/4aCkLCLo7t3Io8+t49FV65LksWodty5dTeF/i/bWFnafPpkZ7a3MmDqRzqmtzJjayoz2icl7+po+ZSLNnh3PbNyoSYKQdAzwH0Az8K2IuKCovhW4CngHyVzUH4+Ip6sdpyXzUuQ72sh3tHH4W3L95a9u2srjq3uTM41V61j50qusWb+RJ7vX07N+I5u29G23rybB9CnZpJEut7ey06QJTJ7YzJSJLUxubWbyxBamTGxmcmv6PrGFiS3uEzGrpqonCEnNwMXAkcAK4F5JiyLi0Uyz04EXI+JNkuYDXwY+Xu1YrbxJE5s5cPedOXD3nberiwh6N25hTe9G1qzfxJr1G5NX70Z6MuvP/G4Da3o38ermrUP6zAnN6k8ckyY2M6W1pT+ptE5oormpiZYm0dwkJjQn7y1NTcl7s9K6pE3xenP2peS9qSlp06TCvpKyQn3yguamJpoETRJK35NXkmCz7+XaSMk8vE39y0JNJcpE/3L/tqRlvsxno6wWZxBzgeUR8RSApO8D84BsgpgHnJ8u/wC4SJKinjpM6pgkOtom0NE2gb06B2+/YeMWel/bwoZNW3hl49bkfdMWNmzc2v/+6uatbNi4hVc2Zd7T9qtefo3NW/vY2hds7utj69ZgS18k62l5YX1LX/3/CBWSTTZ5QJJU6K/LJJz+7fT6ZPFF5dn9FrdVdqNtPq+4Po2hRJvs5wz0dQ24XmKq+1K7LPcpg37+gLWDNxhs+5Ek+OmTJ3Ldme8a9vbl1CJB7AY8m1lfARxcrk1EbJH0MrALsKZ4Z5IWAAsA9thjj0rEaxU2pbWlavNtR8S2CWNrklT6+oKtaV3h1Revt+vrI63vY2sfr7dLy/r6IIC+CCKCvkiW+4J0PYigv3ybNn1BANHflnQ92SYo7CdZLrQr7K+/LPkCX99XWg5sU0bR/rP1yfLr5dnjFv3L6X622YZt1imq37Ztibrtvk9F68UtBl7tj3m7shLtSn3eULcb6LN2ZPvBGwysva0yvz/jvpM6IhYCCyG5i6nG4dgYJ6WXlzwu0GxQtej1WwnsnlmfnZaVbCOpBdiJpLPazMyqpBYJ4l5gb0lvkDQRmA8sKmqzCDglXf4Y8HP3P5iZVVfVLzGlfQrnADeT3OZ6eUQ8IumLwOKIWARcBnxH0nJgLUkSMTOzKqpJH0RE3ATcVFT2hczya8D/qnZcZmb2Oo88MjOzkpwgzMysJCcIMzMryQnCzMxKqqvHfUvqAZ4Z5uYzKDFSewxxfCPj+EbG8Y3MWI5vz4go+VCcukoQIyFpcblnoo8Fjm9kHN/IOL6RGevxleNLTGZmVpIThJmZleQE8bqFtQ5gEI5vZBzfyDi+kRnr8ZXkPggzMyvJZxBmZlaSE4SZmZXUcAlC0jGSHpe0XNJ5JepbJV2b1t8jaU4VY9td0i8kPSrpEUmfKdHmfZJelrQkfX2h1L4qGOPTkh5KP3txiXpJujA9fg9KOqiKse2TOS5LJK2TdG5Rm6oeP0mXS+qW9HCmbLqkWyUtS9+nldn2lLTNMkmnlGpTofj+RdJj6ffvBkk7l9l2wJ+FCsZ3vqSVme/hcWW2HfB3vYLxXZuJ7WlJS8psW/HjN2KRTn/YCC+Sx4s/CewFTAQeAPYtavOnwKXp8nzg2irGNxM4KF1uB54oEd/7gB/V8Bg+DcwYoP444CckU/AeAtxTw+/18ySDgGp2/ID3AgcBD2fKvgKcly6fB3y5xHbTgafS92np8rQqxXcU0JIuf7lUfEP5WahgfOcDfzmE7/+Av+uViq+o/t+AL9Tq+I301WhnEHOB5RHxVERsAr4PzCtqMw+4Ml3+AXCERjKb+A6IiFURcX+63AssJZmfezyZB1wVibuBnSXNrEEcRwBPRsRwR9aPioi4nWROk6zsz9iVwIdLbHo0cGtErI2IF4FbgWOqEV9E3BIRW9LVu0lmfayJMsdvKIbyuz5iA8WX/t04EbhmtD+3WhotQewGPJtZX8H2f4D726S/JC8Du1Qluoz00tbbgXtKVL9L0gOSfiJpv+pGRgC3SLpP0oIS9UM5xtUwn/K/mLU8fgD5iFiVLj8P5Eu0GSvH8TSSM8JSBvtZqKRz0ktgl5e5RDcWjt97gNURsaxMfS2P35A0WoIYFyRNBX4InBsR64qq7ye5bPI24GvAjVUO77CIOAg4Fjhb0nur/PmDUjKV7YeA/yxRXevjt41IrjWMyXvNJX0e2AJcXaZJrX4WLgHeCBwIrCK5jDMWncTAZw9j/nep0RLESmD3zPrstKxkG0ktwE7AC1WJLvnMCSTJ4eqIuL64PiLWRcT6dPkmYIKkGdWKLyJWpu/dwA0kp/JZQznGlXYscH9ErC6uqPXxS60uXHZL37tLtKnpcZR0KnA88Ik0iW1nCD8LFRERqyNia0T0Ad8s87m1Pn4twAnAteXa1Or47YhGSxD3AntLekP6X+Z8YFFRm0VA4Y6RjwE/L/cLMtrSa5aXAUsj4qtl2uxa6BORNJfke1iVBCZpiqT2wjJJZ+bDRc0WAZ9M72Y6BHg5czmlWsr+51bL45eR/Rk7BfivEm1uBo6SNC29hHJUWlZxko4B/hr4UES8UqbNUH4WKhVftk/rI2U+dyi/65X0AeCxiFhRqrKWx2+H1LqXvNovkrtsniC5w+HzadkXSX4ZANpILk0sB34D7FXF2A4judzwILAkfR0HnAmcmbY5B3iE5K6Mu4FDqxjfXunnPpDGUDh+2fgEXJwe34eArip/f6eQ/MHfKVNWs+NHkqhWAZtJroOfTtKndRuwDPgZMD1t2wV8K7PtaenP4XLgU1WMbznJ9fvCz2Dhrr5ZwE0D/SxUKb7vpD9bD5L80Z9ZHF+6vt3vejXiS8uvKPzMZdpW/fiN9OVHbZiZWUmNdonJzMyGyAnCzMxKcoIwM7OSnCDMzKwkJwgzMyvJCcJsB0jaqm2fGDtqTwmVNCf7VFCzWmupdQBm48yrEXFgrYMwqwafQZiNgvTZ/l9Jn+//G0lvSsvnSPp5+mC52yTtkZbn07kWHkhfh6a7apb0TSXzgdwiaVLNvihreE4QZjtmUtElpo9n6l6OiN8DLgL+b1r2NeDKiDiA5KF3F6blFwK/jOShgQeRjKYF2Bu4OCL2A14CPlrRr8ZsAB5JbbYDJK2PiKklyp8G3h8RT6UPXHw+InaRtIbkURCb0/JVETFDUg8wOyI2ZvYxh2QOiL3T9c8BEyLiH6rwpZltx2cQZqMnyizviI2Z5a24n9BqyAnCbPR8PPP+63T5LpIniQJ8ArgjXb4NOAtAUrOknaoVpNlQ+b8Tsx0zqWgS+p9GROFW12mSHiQ5CzgpLfs08G1JfwX0AJ9Kyz8DLJR0OsmZwlkkTwU1GzPcB2E2CtI+iK6IWFPrWMxGiy8xmZlZST6DMDOzknwGYWZmJTlBmJlZSU4QZmZWkhOEmZmV5ARhZmYl/X/Ph2Fg4q69PAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The policy iteration and value iteration both reached the same optimal policy.\n"
     ]
    }
   ],
   "source": [
    "# Compute optimal value function\n",
    "Policy = np.zeros((world.state_size, world.action_size))\n",
    "\n",
    "threshold = 0.001\n",
    "gamma = 0.9\n",
    "V, Policy, epochs = world.policy_iteration(Policy, discount=gamma, threshold=threshold)\n",
    "optimal_policy, optimal_V, epochs = world.value_iteration(discount=gamma, threshold=threshold)\n",
    "\n",
    "# Plot of optimal value and policy functions\n",
    "world.draw_delta_history()\n",
    "\n",
    "if np.all(optimal_policy == Policy):\n",
    "    print(\"The policy iteration and value iteration both reached the same optimal policy.\")"
   ]
  },
  {
   "source": [
    "### c. Monte Carlo RL (15 pts)\n",
    "\n",
    "1. Estimate the optimal value function using Monte Carlo (MC) reinforcement learning. Briefly\n",
    "state how you solved the problem, including any parameters that you set or assumptions\n",
    "you made"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_World(NewGridWorld):\n",
    "    def __init__(self,\n",
    "            absorbing_locs=None,\n",
    "            special_rewards=None,\n",
    "            p_transition=0.45,\n",
    "            use_first_visit=True,\n",
    "            use_epsilon_greedy=True):\n",
    "        super(MC_World, self).__init__(absorbing_locs, special_rewards, p_transition)\n",
    "\n",
    "        self.use_first_visit = use_first_visit\n",
    "        self.use_epsilon_greedy = use_epsilon_greedy\n",
    "        self.reward_episode_tracking = []\n",
    "\n",
    "        self.des_action_tracker = [0, 0]    # Keep track of how often desired action gets chosen\n",
    "\n",
    "    def mc_iterative_optimisation(self, discount=0.45, epsilon_init=0.1, n=5, alpha=0.001, V_optimal=None, decay_alpha=False, decay_eps=True, batch=1):\n",
    "        \"\"\" MC Iterative Learning to Control \"\"\"\n",
    "        # Q = np.zeros((self.state_size, self.action_size))\n",
    "        Q = np.random.rand(self.state_size, self.action_size)\n",
    "        policy = self.get_epsilon_greedy_policy(Q, epsilon_init)\n",
    "\n",
    "        # Trackers\n",
    "        total_returns = []\n",
    "        all_rmse = []\n",
    "        epsilon = epsilon_init\n",
    "        \n",
    "        for run in range(n):\n",
    "            trace = self.run_episode(policy, epsilon)\n",
    "            rewards_forward = [r[2] for r in trace]\n",
    "            rewards_reverse = rewards_forward[::-1]\n",
    "\n",
    "            state_a_tracker = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "            # Rewrite\n",
    "            # for i, (state, action, R) in enumerate(trace):\n",
    "            #     G = 0 \n",
    "            #     Rt = 0\n",
    "            #     if (not state_a_tracker[state, action]) and self.use_first_visit:\n",
    "            #         state_a_tracker[state, action] += 1\n",
    "            #         G = self.get_discounted_rewards(rewards_forward[i::], discount)\n",
    "            #         G = np.sum(G)\n",
    "            #         for _, _, rev_R in reversed(trace[i::]):\n",
    "            #             Rt = discount*Rt + rev_R\n",
    "\n",
    "            #         if decay_alpha:\n",
    "            #             alpha = 1 / state_a_tracker[state, action]\n",
    "                    \n",
    "            #         Q[state, action] += alpha * (G - Q[state, action])\n",
    "            # /Rewrite\n",
    "\n",
    "            ### OLD but works\n",
    "            G = 0\n",
    "            for i, (state, action, R) in enumerate(trace[::-1]):\n",
    "                if state_a_tracker[state, action] and self.use_first_visit:\n",
    "                    continue\n",
    "                state_a_tracker[state, action] += 1\n",
    "\n",
    "                # Find the points in the trace with this (s, a)\n",
    "                reward_idx = self.get_occurence_idx(trace, state, action)\n",
    "                if reward_idx==None:\n",
    "                    # Use current index if first index not found\n",
    "                    reward_idx = [(len(trace)-1) - i]\n",
    "\n",
    "                # Calculate first-visit or every-visit returns \n",
    "                G = rewards_forward[reward_idx[0]:]\n",
    "                G = np.sum(self.get_discounted_rewards(G, discount))\n",
    "                Rt = G / state_a_tracker[state, action]\n",
    "                \n",
    "                if decay_alpha:\n",
    "                    alpha = 1 / state_a_tracker[state, action]\n",
    "\n",
    "                Q[state, action] += alpha * (Rt - Q[state, action])\n",
    "            ### /OLD but works\n",
    "            \n",
    "            if (run % batch) == 0:\n",
    "                policy = self.get_epsilon_greedy_policy(Q, epsilon)\n",
    "            if decay_eps:\n",
    "                epsilon = 1 / (i+1)\n",
    "\n",
    "            # Record rewards\n",
    "            total_return = 0\n",
    "            for s, a, R in trace:   # Forward discounted\n",
    "                total_return = discount*total_return + R\n",
    "            # total_reward = self.get_discounted_rewards(rewards_reverse, discount)\n",
    "            total_returns.append(np.sum(total_return))\n",
    "            if np.any(V_optimal):   # Root Mean Square Error\n",
    "                # V = self.get_optimal_value(Q, policy)\n",
    "                # TODO: Changing V calculation\n",
    "                V = np.max(Q, axis=1)\n",
    "                rmse = self.get_rmse(V, V_optimal)\n",
    "                all_rmse.append(rmse)\n",
    "        \n",
    "        # Value function\n",
    "        V = self.get_optimal_value(Q, policy)\n",
    "\n",
    "        # policy = self.get_greedy_policy(Q)\n",
    "        return policy, V, total_returns, all_rmse\n",
    "\n",
    "    # def mc_batch_optimisation(self, discount=0.45, epsilon_init=0.1, batch=1, n=5, total_runs=10, alpha=0.001, V_optimal=None):\n",
    "    #     \"\"\" MC Batch Learning to Control \"\"\"\n",
    "    #     Q = np.zeros((self.state_size, self.action_size))\n",
    "    #     policy = self.get_epsilon_greedy_policy(Q, epsilon_init)\n",
    "\n",
    "    #     # Trackers\n",
    "    #     total_rewards = []\n",
    "    #     returns = []\n",
    "    #     all_rmse = []\n",
    "    #     epsilon = epsilon_init\n",
    "    #     for run in range(total_runs):\n",
    "    #         for b in range(batch):\n",
    "    #             Returns = [[[] for i in range(self.action_size)] for i in range(self.state_size)]\n",
    "    #             for i in range(n):\n",
    "    #                 # New episode\n",
    "    #                 trace = self.run_episode(policy, epsilon)\n",
    "    #                 rewards_forward = [r[2] for r in trace]\n",
    "    #                 rewards_reverse = rewards_forward[::-1]\n",
    "\n",
    "    #                 total_reward = []\n",
    "    #                 state_a_tracker = np.zeros(self.state_size)\n",
    "    #                 G = 0\n",
    "\n",
    "    #                 # Old\n",
    "    #                 # state_a_tracker = np.zeros((self.state_size, self.action_size))\n",
    "    #                 # for state, action, reward in trace:\n",
    "    #                 #     if self.use_first_visit and (state_a_tracker[state, action] > 0):\n",
    "    #                 #         reward = state_a_tracker[state, action]\n",
    "    #                 #     else:\n",
    "    #                 #         state_a_tracker[state, action] += reward\n",
    "    #                 #     Returns[state][action].append(reward)\n",
    "\n",
    "    #                 for i, (state, action, R) in enumerate(trace[::-1]):\n",
    "\n",
    "    #                     if self.use_first_visit:\n",
    "    #                         reward_idx = self.get_occurence_idx(trace, state)\n",
    "    #                         if reward_idx==None:\n",
    "    #                             # Use current index if first index not found\n",
    "    #                             reward_idx = (len(trace)-1) - i\n",
    "    #                     else:\n",
    "    #                         reward_idx = (len(trace)-1) - i\n",
    "                        \n",
    "    #                     state_a_tracker[state] += 1\n",
    "    #                     # R = rewards_forward[reward_idx]  # / state_a_tracker[state]\n",
    "    #                     R = rewards_reverse[i]\n",
    "\n",
    "    #                     G = G * discount + R\n",
    "\n",
    "    #                     total_reward.append(R)\n",
    "\n",
    "    #                     Q[state, action] = Q[state, action] + alpha * (G - Q[state, action])\n",
    "\n",
    "    #                 epsilon = epsilon / (i+1)\n",
    "\n",
    "    #             # Old\n",
    "    #             # for state in range(self.state_size):\n",
    "    #             #     for action in range(self.action_size):\n",
    "    #             #         G = self.get_discounted_reward(Returns[state][action], discount)\n",
    "    #             #         # Q[state, action] = self.get_discounted_reward(Returns[state][action], discount)\n",
    "    #             #         Q[state, action] = Q[state, action] + alpha * (G - Q[state, action])\n",
    "                \n",
    "    #             # Record rewards\n",
    "    #             total_rewards.append(np.sum(total_reward))\n",
    "    #             returns.append(np.sum(self.get_discounted_rewards(total_reward)))\n",
    "    #             if np.any(V_optimal):   # Root Mean Square Error\n",
    "    #                 V = self.get_optimal_value(Q, policy)\n",
    "    #                 rmse = self.get_rmse(V, V_optimal)\n",
    "    #                 all_rmse.append(rmse)\n",
    "                \n",
    "    #             policy = self.get_epsilon_greedy_policy(Q, epsilon_init)\n",
    "    #     # policy = self.get_greedy_policy(Q)\n",
    "\n",
    "    #     # Value function\n",
    "    #     V = self.get_optimal_value(Q, policy)\n",
    "    #     return policy, V, total_rewards, returns, all_rmse\n",
    "\n",
    "    # def mc_onpolicy_control(self, discount=0.45, epsilon_init=0.1, total_runs=2, alpha=0.001):\n",
    "    #     \"\"\" On-policy epsilon-greedy first-visit MC control algorithm (197/234) \"\"\"\n",
    "    #     # Init\n",
    "    #     episode_num = 0\n",
    "    #     self.reward_episode_tracking = []\n",
    "    #     Q = np.zeros((self.state_size, self.action_size))\n",
    "    #     Returns = [[[] for i in range(self.action_size)] for i in range(self.state_size)]\n",
    "    #     policy = self.get_epsilon_greedy_policy(Q, epsilon_init)  # np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    #     total_rewards = []\n",
    "    #     mean_rewards = []\n",
    "    #     while episode_num <= total_runs:\n",
    "    #         # Generate episode and trace using policy\n",
    "    #         episode_num += 1\n",
    "    #         epsilon = epsilon_init / episode_num\n",
    "\n",
    "    #         current_trace = self.run_episode(policy)\n",
    "    #         state_a_tracker = np.zeros((self.state_size, self.action_size))\n",
    "    #         total_reward = 0\n",
    "    #         G = 0\n",
    "    #         for state, action, reward in current_trace:\n",
    "    #             if self.use_first_visit and (state_a_tracker[state, action] > 0):\n",
    "    #                 reward = state_a_tracker[state, action]\n",
    "    #             else:\n",
    "    #                 state_a_tracker[state, action] += reward\n",
    "    #             total_reward += reward\n",
    "    #             Returns[state][action].append(reward)\n",
    "\n",
    "    #             G = self.get_discounted_reward(Returns[state][action], discount)\n",
    "    #             # G = G * discount + reward\n",
    "\n",
    "    #             Q[state, action] = Q[state, action] + alpha * (G - Q[state, action])\n",
    "\n",
    "    #         policy = self.get_epsilon_greedy_policy(Q, epsilon)\n",
    "\n",
    "    #         # Record rewards\n",
    "    #         total_rewards.append(np.sum(total_reward))\n",
    "    #         mean_rewards.append(np.average(total_reward))\n",
    "    #         # mean_reward = np.sum(Q) / (np.prod(Q.shape))\n",
    "    #         # self.reward_episode_tracking.append(mean_reward)\n",
    "\n",
    "    #     # Value function\n",
    "    #     V = self.get_optimal_value(Q, policy)\n",
    "\n",
    "    #     return policy, V, total_rewards, mean_rewards\n",
    "\n",
    "    # def evaluate_trace(self, Q, Returns, current_trace, alpha, discount):\n",
    "    #     \"\"\" Calculate Q from this trace and store returns \"\"\"\n",
    "        \n",
    "    # def mc_policy_evaluation(self, Policy, V, discount=0.9, batch=10, alpha=0.001):\n",
    "    #     \"\"\" Estimate optimal value function of an unknown MDP \"\"\"\n",
    "    #     traces = []\n",
    "    #     Returns = [[] for i in range(self.state_size)]\n",
    "    #     Vnew = np.copy(V)\n",
    "\n",
    "    #     # Run batch number of episodes\n",
    "    #     for _i in range(batch):\n",
    "    #         # alpha = 1 / epoch\n",
    "    #         current_trace = self.run_episode(Policy)\n",
    "\n",
    "    #         state_a_tracker = np.zeros(self.state_size)\n",
    "    #         for state, action, reward in current_trace:\n",
    "    #             if self.use_first_visit and (state_a_tracker[state] > 0):\n",
    "    #                 reward = state_a_tracker[state, action]\n",
    "    #             else:\n",
    "    #                 state_a_tracker[state, action] += reward\n",
    "    #             Returns[state].append(reward)\n",
    "    #             R_t = self.get_discounted_reward(Returns[state], discount)\n",
    "    #             V[state] += alpha * (R_t - V[state])\n",
    "    #     return V\n",
    "\n",
    "    def get_rmse(self, predictions, targets):\n",
    "        return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "    def get_occurence_idx(self, trace, state, action):\n",
    "        \"\"\" To help with first-visit, get the index of the state in the trace \"\"\"\n",
    "        all_occurences = []\n",
    "        for i, t in enumerate(trace):\n",
    "            if (t[0] == state) and (t[1] == action):\n",
    "                all_occurences.append(i)\n",
    "        return all_occurences\n",
    "\n",
    "    def get_optimal_value(self, Q, policy):\n",
    "        \"\"\" Get final optimal value from Q and policy \"\"\"\n",
    "        V = np.zeros(self.state_size)\n",
    "        for state in range(self.state_size):\n",
    "            optimal_action = int(np.argmax(policy[state, :]))\n",
    "            V[state] = Q[state, optimal_action]\n",
    "        return V\n",
    "\n",
    "    def get_greedy_policy(self, Q):\n",
    "        \"\"\" Find the greedy policy from Q \"\"\"\n",
    "        policy = np.zeros((self.state_size, self.action_size))\n",
    "        for state in range(self.state_size):\n",
    "            # If this state has the same Q for each action, don't update policy\n",
    "            if np.all(Q[state, :] == Q[state, 0]):\n",
    "                policy[state, :] = 1 / len(policy[state, :])\n",
    "                continue\n",
    "\n",
    "            a_optimal = np.argmax(Q[state, :])\n",
    "\n",
    "            if not(self.absorbing[0,state]):\n",
    "                new_policy = np.zeros(self.action_size)\n",
    "                new_policy[np.argmax(Q[state, :])] = 1  # The action that maximises the Q value gets probability 1\n",
    "                policy[state] = new_policy\n",
    "        return policy\n",
    "\n",
    "    def get_epsilon_greedy_policy(self, Q, epsilon):\n",
    "        # Generate soft policy from Q\n",
    "        soft_policy = np.zeros((self.state_size, self.action_size))\n",
    "        for state in range(self.state_size):\n",
    "            # If this state has the same Q for each action, don't update policy TODO: ASSUMPTION\n",
    "            if np.all(Q[state, :] == Q[state, 0]):\n",
    "                soft_policy[state, :] = 1 / len(soft_policy[state, :])\n",
    "                continue\n",
    "\n",
    "            # a_optimal = np.argmax(Q[state, :])\n",
    "            # Have more than 1 a_optimal TODO: ASSUMPTION\n",
    "            a_optimal = np.where(Q[state, :] == np.max(Q[state, :]))\n",
    "            if type(a_optimal) == tuple:\n",
    "                a_optimal = a_optimal[0]\n",
    "\n",
    "            for a in range(self.action_size):\n",
    "                if a in a_optimal:\n",
    "                    soft_policy[state, a] = 1 - epsilon + (epsilon/self.action_size)\n",
    "                else:\n",
    "                    soft_policy[state, a] = epsilon / self.action_size\n",
    "        return soft_policy\n",
    "\n",
    "    def get_discounted_rewards(self, R_trace, discount):\n",
    "        \"\"\" Calculate the total FORWARD discounted reward \"\"\"       # TODO: fxn goes forward now\n",
    "        R_discounted = np.zeros(len(R_trace))\n",
    "        G = 0\n",
    "        for i, R in enumerate(R_trace):\n",
    "            G = R * pow(discount, (i))\n",
    "            # G = G * discount + R\n",
    "            R_discounted[i] = G\n",
    "        return R_discounted\n",
    "\n",
    "    def run_episode(self, Policy, epsilon=0.0001):\n",
    "        \"\"\" Run an episode from the start state to a terminal state \"\"\"\n",
    "        # T = self.get_transition_matrix()\n",
    "\n",
    "        # R_trace = []\n",
    "        trace = []          # For ease of retrieval, append tuple (state, action, reward)\n",
    "\n",
    "        # Take action, go to next state and get reward\n",
    "        while not(trace):\n",
    "            # Initialise state in random valid part of Grid\n",
    "            current_state = self.get_random_start()\n",
    "            while not(self.absorbing[0, current_state]):\n",
    "                # Check for uniform action probability (if all actions have same prob)\n",
    "                desired_action = self.choose_desired_action(current_state, Policy, epsilon)\n",
    "                \n",
    "                action = self.choose_action(current_state, desired_action)\n",
    "\n",
    "                # Get next state & reward\n",
    "                post_state, R_current = self.take_step(current_state, action)\n",
    "\n",
    "                # Log\n",
    "                # R_trace.append(R_current)\n",
    "                # TODO: Assuming that the policy should learn from the desired action\n",
    "                trace.append((current_state, desired_action, R_current))\n",
    "\n",
    "                # Update current state\n",
    "                current_state = post_state\n",
    "                \n",
    "        return trace\n",
    "\n",
    "    def get_random_start(self):\n",
    "        # Only use valid state locations\n",
    "        start = np.random.randint(0, len(self.locs))\n",
    "        while (start in self.absorbing):\n",
    "            start = np.random.randint(0, len(self.locs))\n",
    "        return start\n",
    "\n",
    "    def take_step(self, current_state, action):\n",
    "        \"\"\" Return next state and reward collected for taking this action step \"\"\"\n",
    "        R = self.get_reward_matrix()\n",
    "\n",
    "        post_state = self.neighbors[current_state, action]\n",
    "        post_state = int(post_state)\n",
    "\n",
    "        reward = R[post_state, current_state, action]\n",
    "\n",
    "        return post_state, reward\n",
    "\n",
    "    def choose_desired_action(self, current_state, Policy, epsilon):\n",
    "        \"\"\" Choose the intended action \"\"\"\n",
    "        if (np.all(Policy[current_state, :] == Policy[current_state, 0])):\n",
    "            desired_action = np.random.randint(0, len(Policy[current_state, :]))\n",
    "        elif np.random.rand() < epsilon:\n",
    "            # Early on do more exploring\n",
    "            desired_action = np.random.randint(0, len(Policy[current_state, :]))\n",
    "        else:\n",
    "            desired_action = np.argmax(Policy[current_state, :])\n",
    "        return desired_action\n",
    "\n",
    "    def choose_action(self, current_state, desired_action):\n",
    "        \"\"\" Returns action actually taken \"\"\"\n",
    "        # Possible probabilities of transitions\n",
    "        self_prob = self.p_transition  # self.action_randomizing_array[effect]\n",
    "        neighbor_prob = (1 - self.p_transition) / (self.action_size - 1)\n",
    "\n",
    "        # possible_post_states = [int(self.neighbors[current_state, dir]) for dir in range(self.action_size)]\n",
    "        # # possible_transition_probs = [T[state, current_state, desired_action] for state in possible_post_states]\n",
    "        # possible_transition_probs = np.ones(len(possible_post_states)) * neighbor_prob\n",
    "        possible_transition_probs = np.ones(self.action_size) * neighbor_prob\n",
    "        possible_transition_probs[desired_action] = self_prob\n",
    "        # If probability does not sum to 1, scale\n",
    "        if not(np.sum(possible_transition_probs) == 1):\n",
    "            if np.sum(possible_transition_probs) == 0:\n",
    "                rect_factor = 1 / len(possible_transition_probs)\n",
    "                possible_transition_probs = [prob + rect_factor for prob in possible_transition_probs]\n",
    "            else:\n",
    "                scaling_factor = 1 / np.sum(possible_transition_probs) \n",
    "                possible_transition_probs = [prob * scaling_factor for prob in possible_transition_probs]\n",
    "\n",
    "        # action_chosen = np.random.choice(a=list(range(len(possible_post_states))), p=possible_transition_probs)\n",
    "        action_chosen = np.random.choice(a=list(range(self.action_size)), p=possible_transition_probs)\n",
    "\n",
    "        # Record whether desired action was real action\n",
    "        self.des_action_tracker[0] += 1\n",
    "        self.des_action_tracker[1] += 1 if (desired_action == action_chosen) else 0\n",
    "        return action_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## MC implementation ########\n",
    "p = 0.45  \n",
    "gamma = 0.55\n",
    "epsilon_init = 0.01\n",
    "alpha = 0.01\n",
    "\n",
    "use_first_visit=True\n",
    "visit_text = '(First-visit)' if use_first_visit else '(Every-visit)'\n",
    "decay_alpha = False\n",
    "batch = 1\n",
    "\n",
    "print('MC Parameters: \\np={} \\ngamma={} \\nepsilon={} \\nalpha={} \\nuse_first_visit={}'.format(p, gamma, epsilon_init, alpha, use_first_visit))\n",
    "\n",
    "# Getting Optimal Value from above\n",
    "Policy_optimal = policy_opt_DPvit\n",
    "V_optimal = V_optimal_DPvit\n",
    "print('Optimal value grid')\n",
    "# dp_world.draw_value(V_optimal, title=r'DP: Optimal Value for Grid World, $\\gamma$={} threshold={}'.format(gamma, threshold))\n",
    "\n",
    "\"\"\"\n",
    "# MC\n",
    "mc_world = MC_World(absorbing_locs, special_rewards, p, use_first_visit=use_first_visit)\n",
    "\n",
    "print('\\n2.c.3 Do repeats and mean and std here')\n",
    "start_time = time.time()\n",
    "all_rmse_mc = []\n",
    "all_total_returns_mc = []\n",
    "# total_runs=100  # repeats lol\n",
    "# n = 5000\n",
    "# for i in range(total_runs):     # Averaging\n",
    "#     policy_mc, V_mc, total_returns_mc, rmse_mc = mc_world.mc_iterative_optimisation(discount=gamma, epsilon_init=epsilon_init, n=n, alpha=alpha, V_optimal=V_optimal, batch=batch)\n",
    "#     all_rmse_mc.append(rmse_mc)\n",
    "#     all_total_returns_mc.append(total_returns_mc)\n",
    "# policy, V, total_rewards, returns, rmse = mc_world.mc_batch_optimisation(discount=gamma, epsilon_init=epsilon_init, n=n, alpha=alpha, V_optimal=V_optimal)\n",
    "print('Average RMSE and returns MC')\n",
    "# mc_world.draw_learning_curve(all_rmse_mc, title_text=r'MC: RMSE, {} repeats {} $\\alpha$={} $\\epsilon$={}'.format(total_runs, visit_text, alpha, epsilon_init), axislabels=('Episodes', 'Root Mean Square Error'), save='MC_rmse_ave.png')\n",
    "# mc_world.draw_learning_curve(all_total_returns_mc, title_text=r'MC: Discounted Returns, {} repeats {} $\\alpha$={} $\\epsilon$={}'.format(total_runs, visit_text, alpha, epsilon_init), axislabels=('Episodes', 'Discounted returns'), save='MC_learncurve_ave.png')\n",
    "\n",
    "print('Drawing policy grid')\n",
    "# mc_world.draw_stochastic_policy(policy_mc, V_mc, title=r'MC: Policy {} $\\alpha$={} $\\epsilon$={}'.format(visit_text, alpha, epsilon_init))\n",
    "\n",
    "print('Drawing value grid')\n",
    "# mc_world.draw_value(V_mc, title=r'MC: Value for Grid World, $\\alpha$={} $\\epsilon$={}'.format(alpha, epsilon_init))\n",
    "\n",
    "# print('The desired action was chosen {} percent of the time.'.format(mc_world.des_action_tracker[1]/mc_world.des_action_tracker[0]))\n",
    "print('2.c.3 took {}s'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# Varying alpha and epsilon\n",
    "print('\\n2.c.4 Vary alpha and epsilon')\n",
    "start_time = time.time()\n",
    "\n",
    "rep_policies, rep_values, rep_all_total_returns, rep_all_rmse = [], [], [], []\n",
    "titles_mc = []\n",
    "\n",
    "alpha_collection = [0.01, 0.2, 0.4]\n",
    "epsilon_collection = [0.01, 0.2, 0.8]\n",
    "\n",
    "alphas = alpha_collection * len(epsilon_collection)\n",
    "epsilon_inits = np.sort(epsilon_collection * len(alpha_collection))\n",
    "repeats = 7\n",
    "n = 3000\n",
    "for alpha, epsilon_init in zip(alphas, epsilon_inits):\n",
    "    policies_mc, values_mc, rep_returns_mc, rep_rmse_mc = [], [], [], []\n",
    "    for r in range(repeats):\n",
    "        policy_mc, V_mc, total_returns_mc, rmse_mc = mc_world.mc_iterative_optimisation(discount=gamma, epsilon_init=epsilon_init, n=n, alpha=alpha, V_optimal=V_optimal, batch=batch)\n",
    "        \n",
    "        policies_mc.append(policy_mc)\n",
    "        values_mc.append(V_mc)\n",
    "        rep_returns_mc.append(total_returns_mc)\n",
    "        rep_rmse_mc.append(rmse_mc)\n",
    "    rep_policies.append(policies_mc)\n",
    "    rep_values.append(values_mc)\n",
    "    rep_all_total_returns.append(rep_returns_mc)\n",
    "    rep_all_rmse.append(rep_rmse_mc)\n",
    "    titles_mc.append('Alpha={}, epsilon={}'.format(alpha, epsilon_init))\n",
    "\n",
    "\n",
    "        # mc_world.draw_stochastic_policy(policies[-1], values[-1], titles[-1])\n",
    "# mc_world.draw_stochastic_policy_grid(policies_mc, values_mc, titles_mc, n_columns=3, n_lines=3)\n",
    "# mc_world.draw_deterministic_policy_grid(policies_mc, values_mc, titles_mc, n_columns=len(p_collection), n_lines=len(gamma_collection), save='DP_policy_grid.png')\n",
    "\n",
    "np.save('rep_policies_mc.npy', rep_policies)\n",
    "np.save('rep_values_mc.npy', rep_values)\n",
    "np.save('rep_all_total_returns_mc.npy', rep_all_total_returns)\n",
    "np.save('rep_all_rmse_mc.npy', rep_values)\n",
    "\n",
    "# Varying alpha and epsilon: Averaged\n",
    "for var in range(len(epsilon_collection)):\n",
    "    s = len(epsilon_collection)\n",
    "    mc_world.draw_learningcurve_repvars(rep_all_total_returns[var*s:(var*s)+s], title_text=r'MC: Discounted Returns varying $\\alpha$ and $\\epsilon$, repeats={}'.format(repeats), var_labels=titles_mc[var*s:(var*s)+s], axislabels=('Episodes', 'Returns'), new_fig=False, save='MC_learncurve_varying_e{}_2c4.png'.format(epsilon_collection[var]))\n",
    "    mc_world.draw_learningcurve_repvars(rep_all_rmse[var*s:(var*s)+s], title_text=r'MC: RMSE varying $\\alpha$ and $\\epsilon$, repeats={}'.format(repeats), var_labels=titles_mc[var*s:(var*s)+s], axislabels=('Episodes', 'RMSE'), new_fig=False, save='MC_rmse_varying_e{}_2c4.png'.format(epsilon_collection[var]))\n",
    "\n",
    "print('2.c.4 took {}s'.format(time.time() - start_time))\n",
    "# TODO\n",
    "## In mc policy evaluation, where value is improved, batch is not the right thing to use \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "# TD Learning\n",
    "\n",
    "## SARSA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'MC_World' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f3f7690f8b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m######################## TD #####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTD_World\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMC_World\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     def __init__(self,\n\u001b[1;32m      4\u001b[0m         \u001b[0mabsorbing_locs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mspecial_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MC_World' is not defined"
     ]
    }
   ],
   "source": [
    "######################## TD #####################\n",
    "class TD_World(MC_World):\n",
    "    def __init__(self,\n",
    "        absorbing_locs=None,\n",
    "        special_rewards=None,\n",
    "        p_transition=0.45,\n",
    "        use_first_visit=True,\n",
    "        use_epsilon_greedy=True\n",
    "        ):\n",
    "        super(TD_World, self).__init__(absorbing_locs, special_rewards, p_transition, use_first_visit, use_epsilon_greedy)\n",
    "\n",
    "    def sarsa(self, discount=0.55, epsilon_init=0.1, episode_total=200, alpha=0.4, V_optimal=None):\n",
    "\n",
    "        total_returns = []\n",
    "        all_rmse = []\n",
    "\n",
    "        Q = np.zeros((self.state_size, self.action_size))\n",
    "        epsilon = epsilon_init\n",
    "\n",
    "        for k in range(episode_total):\n",
    "            current_state = self.get_random_start()\n",
    "            desired_action = self.choose_desired_action(current_state, Q, epsilon)\n",
    "\n",
    "            total_reward = []\n",
    "            sarsa_table = []\n",
    "            while not(self.absorbing[0, current_state]):\n",
    "                action = self.choose_action(current_state, desired_action)\n",
    "\n",
    "                post_state, reward = self.take_step(current_state, action)\n",
    "                post_des_action = self.choose_desired_action(post_state, Q, epsilon)\n",
    "                post_action = self.choose_action(post_state, post_des_action)\n",
    "\n",
    "                sarsa_table.append((current_state, action, reward, post_state, post_action))\n",
    "\n",
    "                # if len(sarsa_table) % batch == 0:\n",
    "                #     # Average SARSA\n",
    "                #     sarsa = []\n",
    "\n",
    "                Q[current_state, action] += alpha * (reward + discount * Q[post_state, post_action] - Q[current_state, action])\n",
    "\n",
    "                current_state, desired_action = post_state, post_action\n",
    "                total_reward.append(reward)\n",
    "\n",
    "            epsilon = 1 / (k+1)\n",
    "\n",
    "            # Record rewards\n",
    "            total_return = 0\n",
    "            for R in total_reward:   # Forward discounted\n",
    "                total_return = discount*total_return + R\n",
    "            total_returns.append(np.sum(total_return))\n",
    "            if np.any(V_optimal):   # Root Mean Square Error\n",
    "                policy = self.get_greedy_policy(Q)\n",
    "                V = self.get_optimal_value(Q, policy)\n",
    "                rmse = self.get_rmse(V, V_optimal)\n",
    "                all_rmse.append(rmse)\n",
    "\n",
    "        Policy_sarsa = self.get_epsilon_greedy_policy(Q, epsilon)\n",
    "        V_sarsa = self.get_optimal_value(Q, Policy_sarsa)\n",
    "\n",
    "        return Policy_sarsa, V_sarsa, total_returns, all_rmse\n",
    "\n",
    "    def q_learning(self, discount=0.55, epsilon_init=0.1, episode_total=200, alpha=0.4, V_optimal=None):\n",
    "        \"\"\" Implementation of Q-Learning \"\"\"\n",
    "\n",
    "        total_returns = []\n",
    "        all_rmse = []\n",
    "\n",
    "        Q = np.zeros((self.state_size, self.action_size))\n",
    "        epsilon = epsilon_init\n",
    "\n",
    "        for k in range(episode_total):\n",
    "            current_state = self.get_random_start()\n",
    "\n",
    "            total_reward = []\n",
    "            while not(self.absorbing[0, current_state]):\n",
    "                desired_action = self.choose_desired_action(current_state, Q, epsilon)\n",
    "                action = self.choose_action(current_state, desired_action)\n",
    "\n",
    "                post_state, reward = self.take_step(current_state, action)\n",
    "                \n",
    "                Q[current_state, action] += alpha * (reward + discount * np.argmax(Q[post_state, :]) - Q[current_state, action])\n",
    "\n",
    "                current_state = post_state\n",
    "                total_reward.append(reward)\n",
    "\n",
    "            epsilon = 1 / (k+1)\n",
    "\n",
    "            # Record rewards\n",
    "            total_return = 0\n",
    "            for R in total_reward:   # Forward discounted\n",
    "                total_return = discount*total_return + R\n",
    "            total_returns.append(np.sum(total_return))\n",
    "            if np.any(V_optimal):   # Root Mean Square Error\n",
    "                policy = self.get_greedy_policy(Q)\n",
    "                V = self.get_optimal_value(Q, policy)\n",
    "                rmse = self.get_rmse(V, V_optimal)\n",
    "                all_rmse.append(rmse)\n",
    "\n",
    "        Policy_ql = self.get_epsilon_greedy_policy(Q, epsilon)\n",
    "        V_ql = self.get_optimal_value(Q, Policy_ql)\n",
    "\n",
    "        return Policy_ql, V_ql, total_returns, all_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n2.d.1 TD Learning-estimated Optimal Value function. 2.d.2 Optimal value and policy graphs')\n",
    "p = 0.45\n",
    "td_world = TD_World(absorbing_locs, special_rewards, p, use_first_visit=True, use_epsilon_greedy=True)\n",
    "\n",
    "discount = 0.55\n",
    "epsilon_init = 0.1\n",
    "alpha = 0.4\n",
    "episode_total = 5000\n",
    "repeats = 100\n",
    "\n",
    "\n",
    "\n",
    "print('\\n2.d.4 TD Varying the exploration parameter e and the learning rate α')\n",
    "start_time = time.time()\n",
    "\n",
    "##### SARSA Varying learning rate\n",
    "rep_policies, rep_values, rep_all_total_returns, rep_all_rmse_sarsa = [], [], [], []\n",
    "titles = []\n",
    "\n",
    "alpha_collection = [0.01, 0.2, 0.4]\n",
    "epsilon_collection = [0.01, 0.2, 0.8]\n",
    "alphas = alpha_collection * len(epsilon_collection)\n",
    "epsilon_inits = np.sort(epsilon_collection * len(alpha_collection))\n",
    "\n",
    "batch = 1\n",
    "repeats = 30\n",
    "episode_total = 3000\n",
    "for alpha, epsilon_init in zip(alphas, epsilon_inits):\n",
    "    policies = []\n",
    "    values = []\n",
    "    all_total_rewards = []\n",
    "    all_rmse_sarsa = []\n",
    "    \n",
    "    for rep in range(repeats):\n",
    "        Policy_sarsa, V_sarsa, total_rewards, rmse_sarsa = td_world.sarsa(discount, epsilon_init, episode_total, alpha, V_optimal)\n",
    "        \n",
    "        policies.append(Policy_sarsa)\n",
    "        values.append(V_sarsa)\n",
    "        all_total_rewards.append(total_rewards)\n",
    "        all_rmse_sarsa.append(rmse_sarsa)\n",
    "    # print('{} ... Ran SARSA with alpha={} epsilon={}'.format(rep, alpha, epsilon_init))\n",
    "    titles.append(r'$\\alpha=${} $\\epsilon={}$'.format(alpha, epsilon_init))\n",
    "\n",
    "    rep_policies.append(np.average(policies, axis=0))\n",
    "    rep_values.append(np.average(values, axis=0))\n",
    "    rep_all_total_returns.append(all_total_rewards)\n",
    "    rep_all_rmse_sarsa.append(all_rmse_sarsa)\n",
    "\n",
    "np.save('rep_policies_sarsa.npy', rep_policies)\n",
    "np.save('rep_values_sarsa.npy', rep_values)\n",
    "np.save('rep_all_total_returns_sarsa.npy', rep_all_total_returns)\n",
    "np.save('rep_all_rmse_sarsa.npy', rep_all_rmse_sarsa)\n",
    "\n",
    "# Draw averaged\n",
    "# for i in range(len(alphas)):\n",
    "    # td_world.draw_stochastic_policy(rep_policies[i], rep_values[i], titles[i])  #, n_columns=1, n_lines=3)\n",
    "\n",
    "td_world.draw_stochastic_policy(rep_policies[0], rep_values[0], title='TD SARSA: Policy, {} repeats'.format(repeats), save='TD-S_policy_world.png')\n",
    "td_world.draw_value(rep_values[0], 'TD SARSA: Value function, {} repeats'.format(repeats), save='TD-S_value_world.png')\n",
    "\n",
    "print('\\n2.d.3 TD Learning curve')\n",
    "td_world.draw_learning_curve(rep_all_total_returns[0], title_text=r'TD SARSA: Discounted Returns, {} repeats {} $\\alpha$={} $\\epsilon$={}'.format(repeats, visit_text, alpha_collection[0], epsilon_collection[0]), axislabels=('Episodes', 'Discounted Returns'), save='TD-S_returns.png')\n",
    "td_world.draw_learning_curve(rep_all_rmse_sarsa[0], title_text=r'TD SARSA: RMSE Optimal Value function, {} repeats {} $\\alpha$={} $\\epsilon$={}'.format(repeats, visit_text, alpha_collection[0], epsilon_collection[0]), axislabels=('Episodes', 'Root Mean Square Error'), save='TD-S_rmse.png')\n",
    "\n",
    "\n",
    "for var in range(len(epsilon_collection)):\n",
    "    s = len(epsilon_collection)\n",
    "    td_world.draw_learningcurve_repvars(rep_all_total_returns[var*s:(var*s)+s], title_text=(r'TD SARSA: Discounted Returns varying $\\alpha$ and $\\epsilon$, {} repeats'.format(repeats)), var_labels=titles[var*s:(var*s)+s], axislabels=('Episodes', 'Returns'), new_fig=False, save='TD-S_returns_e{}.png'.format(epsilon_collection[var]))\n",
    "    td_world.draw_learningcurve_repvars(rep_all_rmse_sarsa[var*s:(var*s)+s], title_text=(r'TD SARSA: RMSE varying $\\alpha$ and $\\epsilon$, {} repeats'.format(repeats)), var_labels=titles[var*s:(var*s)+s], axislabels=('Episodes', 'Root Mean Square Error'), new_fig=False, save='TD-S_rmse_e{}.png'.format(epsilon_collection[var]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Q-learning \n",
    "rep_policies, rep_values, rep_all_total_returns_ql, rep_all_rmse_ql = [], [], [], []\n",
    "titles = []\n",
    "\n",
    "repeats = 30\n",
    "episode_total = 3000\n",
    "epsilon_init = 0.01\n",
    "discount = 0.55\n",
    "alpha = 0.0075\n",
    "for rep in range(repeats):\n",
    "    Policy_ql, V_ql, total_rewards_ql, rmse_ql = td_world.q_learning(discount, epsilon_init, episode_total, alpha, V_optimal)\n",
    "    rep_policies.append(Policy_ql)\n",
    "    rep_values.append(V_ql)\n",
    "    rep_all_total_returns_ql.append(total_rewards_ql)\n",
    "    rep_all_rmse_ql.append(rmse_ql)\n",
    "\n",
    "np.save('rep_policies_ql.py', rep_policies)\n",
    "np.save('rep_values_ql.py', rep_values)\n",
    "np.save('rep_all_total_returns_ql.py', rep_all_total_returns_ql)\n",
    "np.save('rep_all_rmse_ql.py', rep_all_rmse_ql)\n",
    "\n",
    "# Q-Learning Plots\n",
    "td_world.draw_stochastic_policy(np.average(rep_policies, axis=0), np.average(rep_values, axis=0))\n",
    "td_world.draw_value(np.average(rep_values, axis=0))\n",
    "td_world.draw_learning_curve(rep_all_total_returns_ql, title_text=r'Q-Learning: Discounted Returns, {} repeats {} $\\alpha$={} $\\epsilon$={}'.format(repeats, visit_text, alpha, epsilon_init), axislabels=('Episodes', 'Discounted returns'), save='TD-Q_returns.png')\n",
    "td_world.draw_learning_curve(rep_all_rmse_ql, title_text=r'Q-Learning: RMSE, {} repeats {} $\\alpha$={} $\\epsilon$={}'.format(repeats, visit_text, alpha, epsilon_init), axislabels=('Episodes', 'Root Mean Square Error'), save='TD-Q_rmse.png')\n",
    "\n",
    "\n",
    "\n",
    "print('2.d.4 took {}s'.format(time.time() - start_time))"
   ]
  }
 ]
}